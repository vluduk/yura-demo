"""
LangChain-based services for advanced AI features.
Includes multi-step business validation and vector RAG.
"""
import os
import json
from typing import List, Dict, Any, Optional
from django.conf import settings

try:
    from langchain.chains import LLMChain, SequentialChain
    from langchain.prompts import PromptTemplate
    from langchain_google_genai import ChatGoogleGenerativeAI
    from langchain.schema import Document
    from langchain_google_genai import GoogleGenerativeAIEmbeddings
    from langchain_community.vectorstores import Chroma
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False


class BusinessValidationChain:
    """Multi-step business idea validation using LangChain."""
    
    def __init__(self):
        if not LANGCHAIN_AVAILABLE:
            raise ImportError("LangChain is not installed. Run: pip install langchain langchain-google-genai")
        
        api_key = getattr(settings, 'GOOGLE_API_KEY', None) or os.environ.get('GOOGLE_API_KEY')
        if not api_key:
            raise ValueError("GOOGLE_API_KEY not configured")
        
        model_name = getattr(settings, 'GOOGLE_LLM_MODEL', 'gemini-2.0-flash-exp')
        self.llm = ChatGoogleGenerativeAI(
            model=model_name,
            google_api_key=api_key,
            temperature=0.3  # Lower temperature for more consistent analysis
        )
    
    def validate_market(self, business_idea: str) -> str:
        """Step 1: Market Analysis"""
        chain = self._create_market_analysis_chain()
        result = chain.run(business_idea=business_idea)
        return result

    def validate_financials(self, business_idea: str, market_analysis: str) -> str:
        """Step 2: Financial Analysis"""
        chain = self._create_financial_analysis_chain()
        result = chain.run(business_idea=business_idea, market_analysis=market_analysis)
        return result

    def validate_skills(self, business_idea: str, user_context: str) -> str:
        """Step 3: Skills Match"""
        chain = self._create_skills_match_chain()
        result = chain.run(business_idea=business_idea, user_context=user_context)
        return result

    def validate_risks(self, business_idea: str, market_analysis: str, financial_analysis: str, skills_match: str) -> str:
        """Step 4: Risk Assessment"""
        chain = self._create_risk_assessment_chain()
        result = chain.run(
            business_idea=business_idea,
            market_analysis=market_analysis,
            financial_analysis=financial_analysis,
            skills_match=skills_match
        )
        return result

    def validate_verdict(self, business_idea: str, market_analysis: str, financial_analysis: str, skills_match: str, risk_assessment: str) -> str:
        """Step 5: Final Verdict"""
        chain = self._create_final_verdict_chain()
        result = chain.run(
            business_idea=business_idea,
            market_analysis=market_analysis,
            financial_analysis=financial_analysis,
            skills_match=skills_match,
            risk_assessment=risk_assessment
        )
        return result

    def validate_idea(self, business_idea: str, user_context: str) -> Dict[str, Any]:
        """
        Perform multi-step validation of a business idea.
        DEPRECATED: Use individual step methods for interactive validation.
        """
        
        # Step 1: Market Analysis
        market_chain = self._create_market_analysis_chain()
        
        # Step 2: Financial Analysis
        financial_chain = self._create_financial_analysis_chain()
        
        # Step 3: Skills Match
        skills_chain = self._create_skills_match_chain()
        
        # Step 4: Risk Assessment
        risk_chain = self._create_risk_assessment_chain()
        
        # Step 5: Final Verdict
        verdict_chain = self._create_final_verdict_chain()
        
        # Execute sequential chain
        overall_chain = SequentialChain(
            chains=[market_chain, financial_chain, skills_chain, risk_chain, verdict_chain],
            input_variables=["business_idea", "user_context"],
            output_variables=[
                "market_analysis",
                "financial_analysis", 
                "skills_match",
                "risk_assessment",
                "final_verdict"
            ],
            verbose=False
        )
        
        result = overall_chain({
            "business_idea": business_idea,
            "user_context": user_context
        })
        
        return result
    
    def _create_market_analysis_chain(self) -> LLMChain:
        """Chain for market analysis."""
        template = """–ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π—Ç–µ —Ä–∏–Ω–∫–æ–≤—É –ø—Ä–∏–≤–∞–±–ª–∏–≤—ñ—Å—Ç—å –±—ñ–∑–Ω–µ—Å-—ñ–¥–µ—ó.

–ë–Ü–ó–ù–ï–°-–Ü–î–ï–Ø: {business_idea}

–ó–ê–í–î–ê–ù–ù–Ø:
1. –ß–∏ —ñ—Å–Ω—É—î —Ä–µ–∞–ª—å–Ω–∏–π –ø–æ–ø–∏—Ç –Ω–∞ —Ü–µ–π –ø—Ä–æ–¥—É–∫—Ç/–ø–æ—Å–ª—É–≥—É?
2. –•—Ç–æ —Ü—ñ–ª—å–æ–≤–∞ –∞—É–¥–∏—Ç–æ—Ä—ñ—è? (–¥–µ–º–æ–≥—Ä–∞—Ñ—ñ—è, –ø–æ—Ç—Ä–µ–±–∏)
3. –ù–∞—Å–∫—ñ–ª—å–∫–∏ –≤–µ–ª–∏–∫–∏–π —Ä–∏–Ω–æ–∫? (–ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω—ñ –∫–ª—ñ—î–Ω—Ç–∏)
4. –•—Ç–æ –æ—Å–Ω–æ–≤–Ω—ñ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∏?
5. –Ø–∫–∞ —É–Ω—ñ–∫–∞–ª—å–Ω–∞ —Ü—ñ–Ω–Ω—ñ—Å—Ç—å –ø—Ä–æ–ø–æ–∑–∏—Ü—ñ—ó?

–ù–∞–¥–∞–π—Ç–µ —Å—Ç–∏—Å–ª–∏–π –∞–Ω–∞–ª—ñ–∑ (150-200 —Å–ª—ñ–≤) –∑ –ö–û–ù–ö–†–ï–¢–ù–ò–ú–ò –æ—Ü—ñ–Ω–∫–∞–º–∏.

–ê–Ω–∞–ª—ñ–∑ —Ä–∏–Ω–∫—É:"""
        
        prompt = PromptTemplate(
            input_variables=["business_idea"],
            template=template
        )
        
        return LLMChain(
            llm=self.llm,
            prompt=prompt,
            output_key="market_analysis"
        )
    
    def _create_financial_analysis_chain(self) -> LLMChain:
        """Chain for financial viability analysis."""
        template = """–ù–∞ –æ—Å–Ω–æ–≤—ñ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É —Ä–∏–Ω–∫—É, –æ—Ü—ñ–Ω—ñ—Ç—å —Ñ—ñ–Ω–∞–Ω—Å–æ–≤—É –∂–∏—Ç—Ç—î–∑–¥–∞—Ç–Ω—ñ—Å—Ç—å.

–ë–Ü–ó–ù–ï–°-–Ü–î–ï–Ø: {business_idea}
–ê–ù–ê–õ–Ü–ó –†–ò–ù–ö–£: {market_analysis}

–ó–ê–í–î–ê–ù–ù–Ø:
1. –ü—Ä–∏–±–ª–∏–∑–Ω—ñ –ø–æ—á–∞—Ç–∫–æ–≤—ñ –≤–∏—Ç—Ä–∞—Ç–∏ (–º—ñ–Ω—ñ–º—É–º/–º–∞–∫—Å–∏–º—É–º)?
2. –ü–æ—Å—Ç—ñ–π–Ω—ñ —â–æ–º—ñ—Å—è—á–Ω—ñ –≤–∏—Ç—Ä–∞—Ç–∏?
3. –†–µ–∞–ª—ñ—Å—Ç–∏—á–Ω–∞ –º–æ–¥–µ–ª—å –¥–æ—Ö–æ–¥—ñ–≤?
4. –ö–æ–ª–∏ –æ—á—ñ–∫—É—î—Ç—å—Å—è –±–µ–∑–∑–±–∏—Ç–∫–æ–≤—ñ—Å—Ç—å?
5. –ü–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∞ —Ä–µ–Ω—Ç–∞–±–µ–ª—å–Ω—ñ—Å—Ç—å (ROI)?

–ù–∞–¥–∞–π—Ç–µ –ö–û–ù–ö–†–ï–¢–ù–Ü —á–∏—Å–ª–∞ —Ç–∞ —Ä–µ–∞–ª—ñ—Å—Ç–∏—á–Ω—ñ –æ—Ü—ñ–Ω–∫–∏ (150-200 —Å–ª—ñ–≤).

–§—ñ–Ω–∞–Ω—Å–æ–≤–∏–π –∞–Ω–∞–ª—ñ–∑:"""
        
        prompt = PromptTemplate(
            input_variables=["business_idea", "market_analysis"],
            template=template
        )
        
        return LLMChain(
            llm=self.llm,
            prompt=prompt,
            output_key="financial_analysis"
        )
    
    def _create_skills_match_chain(self) -> LLMChain:
        """Chain for matching user skills to business requirements."""
        template = """–û—Ü—ñ–Ω—ñ—Ç—å –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ—Å—Ç—å –Ω–∞–≤–∏—á–æ–∫ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ –≤–∏–º–æ–≥–∞–º –±—ñ–∑–Ω–µ—Å—É.

–ë–Ü–ó–ù–ï–°-–Ü–î–ï–Ø: {business_idea}
–ü–†–û–§–Ü–õ–¨ –ö–û–†–ò–°–¢–£–í–ê–ß–ê: {user_context}

–ó–ê–í–î–ê–ù–ù–Ø:
1. –Ø–∫—ñ –∫–ª—é—á–æ–≤—ñ –Ω–∞–≤–∏—á–∫–∏ –ø–æ—Ç—Ä—ñ–±–Ω—ñ –¥–ª—è —Ü—å–æ–≥–æ –±—ñ–∑–Ω–µ—Å—É?
2. –Ø–∫—ñ –Ω–∞–≤–∏—á–∫–∏ —î —É –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ –∑ –ø—Ä–æ—Ñ—ñ–ª—é?
3. –©–æ –í–Ü–î–ü–û–í–Ü–î–ê–Ñ –≤–∏–º–æ–≥–∞–º? (—Å–∏–ª—å–Ω—ñ —Å—Ç–æ—Ä–æ–Ω–∏)
4. –Ø–∫—ñ –ö–†–ò–¢–ò–ß–ù–Ü –ü–†–û–ì–ê–õ–ò–ù–ò –≤ –Ω–∞–≤–∏—á–∫–∞—Ö?
5. –ß–∏ –º–æ–∂–Ω–∞ –∑–∞–ø–æ–≤–Ω–∏—Ç–∏ –ø—Ä–æ–≥–∞–ª–∏–Ω–∏? –Ø–∫?

–ù–∞–¥–∞–π—Ç–µ —á–µ—Å–Ω—É –æ—Ü—ñ–Ω–∫—É –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ—Å—Ç—ñ (100-150 —Å–ª—ñ–≤).

–û—Ü—ñ–Ω–∫–∞ –Ω–∞–≤–∏—á–æ–∫:"""
        
        prompt = PromptTemplate(
            input_variables=["business_idea", "user_context"],
            template=template
        )
        
        return LLMChain(
            llm=self.llm,
            prompt=prompt,
            output_key="skills_match"
        )
    
    def _create_risk_assessment_chain(self) -> LLMChain:
        """Chain for risk assessment."""
        template = """–ù–∞ –æ—Å–Ω–æ–≤—ñ –≤—Å—ñ—Ö –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö –∞–Ω–∞–ª—ñ–∑—ñ–≤, –≤–∏–∑–Ω–∞—á—Ç–µ –∫–ª—é—á–æ–≤—ñ —Ä–∏–∑–∏–∫–∏.

–ë–Ü–ó–ù–ï–°-–Ü–î–ï–Ø: {business_idea}
–†–ò–ù–û–ö: {market_analysis}
–§–Ü–ù–ê–ù–°–ò: {financial_analysis}
–ù–ê–í–ò–ß–ö–ò: {skills_match}

–ó–ê–í–î–ê–ù–ù–Ø:
1. –¢–û–ü-3 –Ω–∞–π–±—ñ–ª—å—à–∏—Ö —Ä–∏–∑–∏–∫–∏ –¥–ª—è —Ü—å–æ–≥–æ –±—ñ–∑–Ω–µ—Å—É?
2. –Ø–∫ –º—ñ—Ç–∏–≥—É–≤–∞—Ç–∏ –∫–æ–∂–µ–Ω —Ä–∏–∑–∏–∫?
3. –Ø–∫—ñ "—á–µ—Ä–≤–æ–Ω—ñ –ø—Ä–∞–ø–æ—Ä—Ü—ñ" –≤–∞—Ä—Ç–æ –≤—Ä–∞—Ö–æ–≤—É–≤–∞—Ç–∏?
4. –ü–ª–∞–Ω –ë —è–∫—â–æ –æ—Å–Ω–æ–≤–Ω–∞ —ñ–¥–µ—è –Ω–µ —Å–ø—Ä–∞—Ü—é—î?

–ù–∞–¥–∞–π—Ç–µ –ø—Ä–∞–∫—Ç–∏—á–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ —Ä–∏–∑–∏–∫—ñ–≤ (150-200 —Å–ª—ñ–≤).

–û—Ü—ñ–Ω–∫–∞ —Ä–∏–∑–∏–∫—ñ–≤:"""
        
        prompt = PromptTemplate(
            input_variables=["business_idea", "market_analysis", "financial_analysis", "skills_match"],
            template=template
        )
        
        return LLMChain(
            llm=self.llm,
            prompt=prompt,
            output_key="risk_assessment"
        )
    
    def _create_final_verdict_chain(self) -> LLMChain:
        """Chain for final recommendation."""
        template = """–ù–∞ –æ—Å–Ω–æ–≤—ñ –í–°–Ü–• –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö –∞–Ω–∞–ª—ñ–∑—ñ–≤, –Ω–∞–¥–∞–π—Ç–µ —Ñ—ñ–Ω–∞–ª—å–Ω–∏–π –≤–µ—Ä–¥–∏–∫—Ç.

–ë–Ü–ó–ù–ï–°-–Ü–î–ï–Ø: {business_idea}

–ê–ù–ê–õ–Ü–ó–ò:
–†–∏–Ω–æ–∫: {market_analysis}
–§—ñ–Ω–∞–Ω—Å–∏: {financial_analysis}
–ù–∞–≤–∏—á–∫–∏: {skills_match}
–†–∏–∑–∏–∫–∏: {risk_assessment}

–ó–ê–í–î–ê–ù–ù–Ø:
1. –ó–∞–≥–∞–ª—å–Ω–∞ –æ—Ü—ñ–Ω–∫–∞ —ñ–¥–µ—ó: –†–ï–ö–û–ú–ï–ù–î–£–Æ / –ó –û–ë–ï–†–ï–ñ–ù–Ü–°–¢–Æ / –ù–ï –†–ï–ö–û–ú–ï–ù–î–£–Æ
2. –ß–æ–º—É —Å–∞–º–µ —Ç–∞–∫–∞ –æ—Ü—ñ–Ω–∫–∞? (2-3 –∫–ª—é—á–æ–≤—ñ –ø—Ä–∏—á–∏–Ω–∏)
3. –©–û –†–û–ë–ò–¢–ò –î–ê–õ–Ü? (–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ –Ω–∞—Å—Ç—É–ø–Ω—ñ –∫—Ä–æ–∫–∏)
4. –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ñ –ø—ñ–¥—Ö–æ–¥–∏ —è–∫—â–æ —î —Å—É–º–Ω—ñ–≤–∏?

–ë—É–¥—å—Ç–µ —á–µ—Å–Ω–∏–º–∏ —ñ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–∏–º–∏. –Ø–∫—â–æ —ñ–¥–µ—è —Å–ª–∞–±–∫–∞, –∫—Ä–∞—â–µ —Å–∫–∞–∑–∞—Ç–∏ —Ü–µ –∑–∞—Ä–∞–∑.

–§—ñ–Ω–∞–ª—å–Ω–∏–π –≤–µ—Ä–¥–∏–∫—Ç:"""
        
        prompt = PromptTemplate(
            input_variables=[
                "business_idea",
                "market_analysis",
                "financial_analysis",
                "skills_match",
                "risk_assessment"
            ],
            template=template
        )
        
        return LLMChain(
            llm=self.llm,
            prompt=prompt,
            output_key="final_verdict"
        )
    
    def format_validation_response(self, validation_result: Dict[str, Any]) -> str:
        """Format validation results into a readable response."""
        
        response = f"""üìä –î–ï–¢–ê–õ–¨–ù–ò–ô –ê–ù–ê–õ–Ü–ó –ë–Ü–ó–ù–ï–°-–Ü–î–ï–á

üéØ –ê–ù–ê–õ–Ü–ó –†–ò–ù–ö–£
{validation_result['market_analysis']}

üí∞ –§–Ü–ù–ê–ù–°–û–í–ê –û–¶–Ü–ù–ö–ê
{validation_result['financial_analysis']}

üõ† –í–Ü–î–ü–û–í–Ü–î–ù–Ü–°–¢–¨ –ù–ê–í–ò–ß–û–ö
{validation_result['skills_match']}

‚ö†Ô∏è –û–¶–Ü–ù–ö–ê –†–ò–ó–ò–ö–Ü–í
{validation_result['risk_assessment']}

‚úÖ –§–Ü–ù–ê–õ–¨–ù–ò–ô –í–ï–†–î–ò–ö–¢
{validation_result['final_verdict']}
"""
        return response


class VectorRAG:
    """Vector-based Retrieval Augmented Generation for learning mode."""
    
    def __init__(self, persist_directory: str = "/tmp/chroma_db"):
        if not LANGCHAIN_AVAILABLE:
            raise ImportError("LangChain is not installed")
        
        api_key = getattr(settings, 'GOOGLE_API_KEY', None) or os.environ.get('GOOGLE_API_KEY')
        if not api_key:
            raise ValueError("GOOGLE_API_KEY not configured")
        
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model="models/embedding-001",
            google_api_key=api_key
        )
        self.persist_directory = persist_directory
        self.vectorstore = None
    
    def initialize_vectorstore(self, force_refresh: bool = False):
        """Initialize or load existing vector store."""
        try:
            if not force_refresh:
                # Try to load existing vectorstore
                self.vectorstore = Chroma(
                    persist_directory=self.persist_directory,
                    embedding_function=self.embeddings
                )
            else:
                # Refresh from database
                self._refresh_vectorstore()
        except Exception:
            # Create new if loading fails
            self._refresh_vectorstore()
    
    def _refresh_vectorstore(self):
        """Refresh vectorstore from database."""
        from api.models.knowledge import KnowledgeDocument
        from api.models.article import Article
        
        documents = []
        
        # Load KnowledgeDocuments
        knowledge_docs = KnowledgeDocument.objects.all()
        for doc in knowledge_docs:
            documents.append(Document(
                page_content=doc.raw_text_content,
                metadata={
                    "title": doc.title,
                    "source": doc.source_url or "",
                    "type": "knowledge_document",
                    "id": str(doc.id)
                }
            ))
        
        # Load published Articles
        articles = Article.objects.filter(is_published=True)
        for article in articles:
            documents.append(Document(
                page_content=article.content,
                metadata={
                    "title": article.title,
                    "source": f"/articles/{article.slug}",
                    "type": "article",
                    "id": str(article.id)
                }
            ))
        
        if documents:
            self.vectorstore = Chroma.from_documents(
                documents=documents,
                embedding=self.embeddings,
                persist_directory=self.persist_directory
            )
            self.vectorstore.persist()
    
    def search(self, query: str, k: int = 3) -> List[Dict[str, Any]]:
        """
        Semantic search for relevant documents.
        
        Returns list of dicts with:
        - title: Document title
        - content: Relevant snippet
        - source: URL or path
        - type: 'knowledge_document' or 'article'
        - relevance_score: Similarity score
        """
        if not self.vectorstore:
            self.initialize_vectorstore()
        
        # Perform similarity search with scores
        results = self.vectorstore.similarity_search_with_score(query, k=k)
        
        formatted_results = []
        for doc, score in results:
            # Extract snippet (first 300 chars)
            content = doc.page_content[:300]
            if len(doc.page_content) > 300:
                content += "..."
            
            formatted_results.append({
                'title': doc.metadata.get('title', 'Untitled'),
                'content': content,
                'source': doc.metadata.get('source', ''),
                'type': doc.metadata.get('type', 'unknown'),
                'relevance_score': float(1 - score)  # Convert distance to similarity
            })
        
        return formatted_results
    
    def format_rag_context(self, results: List[Dict[str, Any]]) -> str:
        """Format search results for LLM context."""
        if not results:
            return ""
        
        context = "\n\n–†–ï–õ–ï–í–ê–ù–¢–ù–Ü –ú–ê–¢–ï–†–Ü–ê–õ–ò –ó –ë–ê–ó–ò –ó–ù–ê–ù–¨:\n"
        for i, result in enumerate(results, 1):
            context += f"\n{i}. [{result['type'].upper()}] {result['title']}\n"
            context += f"   {result['content']}\n"
            if result['source']:
                context += f"   –î–∂–µ—Ä–µ–ª–æ: {result['source']}\n"
            context += f"   –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å: {result['relevance_score']:.2%}\n"
        
        return context
