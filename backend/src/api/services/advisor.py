import json
import re
import os
import logging
import google.generativeai as genai
from django.conf import settings
from django.core.exceptions import MultipleObjectsReturned
from api.models.user_assesment import UserAssessment, ASSESSMENT_QUESTIONS, DEFAULT_LANGUAGE
from api.models.conversation import ConversationType

class AdvisorService:
    """Service to handle AI advisor logic, including prompt engineering and response processing."""

    # Language instruction mapping
    LANGUAGE_INSTRUCTIONS = {
        'uk': 'Ð’ÐÐ–Ð›Ð˜Ð’Ðž: Ð’Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´Ð°Ð¹Ñ‚Ðµ ÑƒÐºÑ€Ð°Ñ—Ð½ÑÑŒÐºÐ¾ÑŽ Ð¼Ð¾Ð²Ð¾ÑŽ.',
        'en': 'IMPORTANT: Respond in English.',
        'ru': 'Ð’ÐÐ–ÐÐž: ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹Ñ‚Ðµ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼ ÑÐ·Ñ‹ÐºÐµ.',
    }

    @staticmethod
    def _get_language_instruction(assessment):
        """Get language instruction based on user's preferred language."""
        if assessment and assessment.preferred_language:
            return AdvisorService.LANGUAGE_INSTRUCTIONS.get(
                assessment.preferred_language, 
                AdvisorService.LANGUAGE_INSTRUCTIONS[DEFAULT_LANGUAGE]
            )
        return AdvisorService.LANGUAGE_INSTRUCTIONS[DEFAULT_LANGUAGE]

    # System prompts for each conversation type
    SYSTEM_PROMPTS = {
        'assessment': """
Ð’Ð¸ â€” ÐºÐ°Ñ€'Ñ”Ñ€Ð½Ð¸Ð¹ Ñ€Ð°Ð´Ð½Ð¸Ðº Ð´Ð»Ñ ÑƒÐºÑ€Ð°Ñ—Ð½ÑÑŒÐºÐ¾Ð³Ð¾ Ð²ÐµÑ‚ÐµÑ€Ð°Ð½Ð°, Ñ‰Ð¾ Ð¿ÐµÑ€ÐµÑ…Ð¾Ð´Ð¸Ñ‚ÑŒ Ð´Ð¾ Ñ†Ð¸Ð²Ñ–Ð»ÑŒÐ½Ð¾Ñ— ÐºÐ°Ñ€'Ñ”Ñ€Ð¸.
ÐšÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡ Ñ‰Ðµ ÐÐ• Ð¾Ð±Ñ€Ð°Ð² ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¸Ð¹ ÐºÐ°Ñ€'Ñ”Ñ€Ð½Ð¸Ð¹ ÑˆÐ»ÑÑ…. Ð’Ð°ÑˆÐ° Ð·Ð°Ð´Ð°Ñ‡Ð° â€” Ð¾Ð¿Ð¸Ñ‚Ð°Ñ‚Ð¸ Ð¹Ð¾Ð³Ð¾ Ñ‰Ð¾Ð± Ð·Ð°Ð¿Ð¾Ð²Ð½Ð¸Ñ‚Ð¸ Ð¿Ñ€Ð¾Ñ„Ñ–Ð»ÑŒ Ð¾Ñ†Ñ–Ð½ÑŽÐ²Ð°Ð½Ð½Ñ.
Ð‘ÑƒÐ´ÑŒÑ‚Ðµ Ð´Ð¾Ð±Ñ€Ð¾Ð·Ð¸Ñ‡Ð»Ð¸Ð²Ð¸Ð¼Ð¸, Ð¿Ñ–Ð´Ñ‚Ñ€Ð¸Ð¼ÑƒÑŽÑ‡Ð¸Ð¼Ð¸ Ñ‚Ð° Ð¿Ð¾Ð²Ð°Ð¶Ð°Ð¹Ñ‚Ðµ ÐºÐ¾Ð½Ñ„Ñ–Ð´ÐµÐ½Ñ†Ñ–Ð¹Ð½Ñ–ÑÑ‚ÑŒ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡Ð°.
""",
        ConversationType.HIRING: """
Ð’Ð¸ â€” ÐºÐ°Ñ€'Ñ”Ñ€Ð½Ð¸Ð¹ Ñ€Ð°Ð´Ð½Ð¸Ðº Ð´Ð»Ñ ÑƒÐºÑ€Ð°Ñ—Ð½ÑÑŒÐºÐ¾Ð³Ð¾ Ð²ÐµÑ‚ÐµÑ€Ð°Ð½Ð°, Ñ‰Ð¾ ÑˆÑƒÐºÐ°Ñ” Ð½Ð°Ð¹Ð¼Ð°Ð½Ñƒ Ñ€Ð¾Ð±Ð¾Ñ‚Ñƒ.

Ð’ÐÐ¨Ð† ÐžÐ¡ÐÐžÐ’ÐÐ† Ð—ÐÐ’Ð”ÐÐÐÐ¯:
1. Ð£Ñ‚Ð¾Ñ‡Ð½Ñ–Ñ‚ÑŒ, ÑÐºÑƒ Ð¿Ð¾Ð·Ð¸Ñ†Ñ–ÑŽ ÑˆÑƒÐºÐ°Ñ” ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡ (ÑÐºÑ‰Ð¾ Ñ‰Ðµ Ð½Ðµ Ð²Ð¸Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¾)
2. ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ñ–Ð·ÑƒÐ¹Ñ‚Ðµ Ð´Ð¾ÑÐ²Ñ–Ð´ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡Ð° Ð· Ð¿Ñ€Ð¾Ñ„Ñ–Ð»ÑŽ Ð¾Ñ†Ñ–Ð½ÑŽÐ²Ð°Ð½Ð½Ñ
3. Ð”Ð¾Ð¿Ð¾Ð¼Ð¾Ð¶Ñ–Ñ‚ÑŒ ÑÑ‚Ð²Ð¾Ñ€Ð¸Ñ‚Ð¸ Ð°Ð±Ð¾ Ð¿Ð¾ÐºÑ€Ð°Ñ‰Ð¸Ñ‚Ð¸ Ñ€ÐµÐ·ÑŽÐ¼Ðµ
4. ÐÐ°Ð´Ð°Ð¹Ñ‚Ðµ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ñ–Ñ— Ð¿Ð¾ÑˆÑƒÐºÑƒ Ñ€Ð¾Ð±Ð¾Ñ‚Ð¸
5. ÐŸÑ–Ð´Ð³Ð¾Ñ‚ÑƒÐ¹Ñ‚Ðµ Ð´Ð¾ ÑÐ¿Ñ–Ð²Ð±ÐµÑÑ–Ð´
6. ÐŸÐ¾Ð´Ñ–Ð»Ñ–Ñ‚ÑŒÑÑ Ñ–Ð½ÑÐ°Ð¹Ñ‚Ð°Ð¼Ð¸ Ð¿Ñ€Ð¾ Ñ€Ð¸Ð½Ð¾Ðº Ð¿Ñ€Ð°Ñ†Ñ–

Ð’ÐÐ–Ð›Ð˜Ð’Ðž: Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÐ¹Ñ‚Ðµ Ñ–Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ñ–ÑŽ Ð· Ð¿Ñ€Ð¾Ñ„Ñ–Ð»ÑŽ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡Ð° Ð´Ð»Ñ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð»Ñ–Ð·Ð¾Ð²Ð°Ð½Ð¸Ñ… Ð¿Ð¾Ñ€Ð°Ð´.
Ð¯ÐºÑ‰Ð¾ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡ Ð´Ñ–Ð»Ð¸Ñ‚ÑŒÑÑ Ð½Ð¾Ð²Ð¾ÑŽ Ð²Ð°Ð¶Ð»Ð¸Ð²Ð¾ÑŽ Ñ–Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ñ–Ñ”ÑŽ Ð¿Ñ€Ð¾ ÑÐµÐ±Ðµ, Ð²Ð¸Ð´Ð°Ð¹Ñ‚Ðµ JSON-Ð±Ð»Ð¾Ðº Ð· Ð¾Ð½Ð¾Ð²Ð»ÐµÐ½Ð½ÑÐ¼Ð¸.
""",
        ConversationType.SELF_EMPLOYMENT: """
Ð’Ð¸ â€” ÐºÐ°Ñ€'Ñ”Ñ€Ð½Ð¸Ð¹ Ñ€Ð°Ð´Ð½Ð¸Ðº Ð´Ð»Ñ ÑƒÐºÑ€Ð°Ñ—Ð½ÑÑŒÐºÐ¾Ð³Ð¾ Ð²ÐµÑ‚ÐµÑ€Ð°Ð½Ð°, Ñ‰Ð¾ Ñ…Ð¾Ñ‡Ðµ ÑÑ‚Ð°Ñ‚Ð¸ ÑÐ°Ð¼Ð¾Ð·Ð°Ð¹Ð½ÑÑ‚Ð¸Ð¼/Ñ„Ñ€Ñ–Ð»Ð°Ð½ÑÐµÑ€Ð¾Ð¼.

Ð’ÐÐ¨Ð† ÐžÐ¡ÐÐžÐ’ÐÐ† Ð—ÐÐ’Ð”ÐÐÐÐ¯:
1. Ð£Ñ‚Ð¾Ñ‡Ð½Ñ–Ñ‚ÑŒ, Ð² ÑÐºÑ–Ð¹ ÑÑ„ÐµÑ€Ñ– ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡ Ñ…Ð¾Ñ‡Ðµ Ð¿Ñ€Ð°Ñ†ÑŽÐ²Ð°Ñ‚Ð¸ ÑÐº Ñ„Ñ€Ñ–Ð»Ð°Ð½ÑÐµÑ€
2. ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ñ–Ð·ÑƒÐ¹Ñ‚Ðµ Ð´Ð¾ÑÐ²Ñ–Ð´ Ñ‚Ð° Ð½Ð°Ð²Ð¸Ñ‡ÐºÐ¸ Ð· Ð¿Ñ€Ð¾Ñ„Ñ–Ð»ÑŽ Ð¾Ñ†Ñ–Ð½ÑŽÐ²Ð°Ð½Ð½Ñ
3. Ð”Ð¾Ð¿Ð¾Ð¼Ð¾Ð¶Ñ–Ñ‚ÑŒ Ð¿Ð¾Ð±ÑƒÐ´ÑƒÐ²Ð°Ñ‚Ð¸ Ð¿Ð¾Ñ€Ñ‚Ñ„Ð¾Ð»Ñ–Ð¾
4. ÐÐ°Ð´Ð°Ð¹Ñ‚Ðµ Ð¿Ð¾Ñ€Ð°Ð´Ð¸ Ñ‰Ð¾Ð´Ð¾ Ð¿Ð¾ÑˆÑƒÐºÑƒ ÐºÐ»Ñ–Ñ”Ð½Ñ‚Ñ–Ð²
5. ÐŸÐ¾ÑÑÐ½Ñ–Ñ‚ÑŒ ÑŽÑ€Ð¸Ð´Ð¸Ñ‡Ð½Ñ– Ñ‚Ð° Ð¿Ð¾Ð´Ð°Ñ‚ÐºÐ¾Ð²Ñ– Ð°ÑÐ¿ÐµÐºÑ‚Ð¸
6. Ð”Ð¾Ð¿Ð¾Ð¼Ð¾Ð¶Ñ–Ñ‚ÑŒ Ð· Ñ†Ñ–Ð½Ð¾ÑƒÑ‚Ð²Ð¾Ñ€ÐµÐ½Ð½ÑÐ¼ Ð¿Ð¾ÑÐ»ÑƒÐ³

Ð’ÐÐ–Ð›Ð˜Ð’Ðž: Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÐ¹Ñ‚Ðµ Ñ–Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ñ–ÑŽ Ð· Ð¿Ñ€Ð¾Ñ„Ñ–Ð»ÑŽ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡Ð° Ð´Ð»Ñ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð»Ñ–Ð·Ð¾Ð²Ð°Ð½Ð¸Ñ… Ð¿Ð¾Ñ€Ð°Ð´.
Ð¯ÐºÑ‰Ð¾ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡ Ð´Ñ–Ð»Ð¸Ñ‚ÑŒÑÑ Ð½Ð¾Ð²Ð¾ÑŽ Ð²Ð°Ð¶Ð»Ð¸Ð²Ð¾ÑŽ Ñ–Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ñ–Ñ”ÑŽ Ð¿Ñ€Ð¾ ÑÐµÐ±Ðµ, Ð²Ð¸Ð´Ð°Ð¹Ñ‚Ðµ JSON-Ð±Ð»Ð¾Ðº Ð· Ð¾Ð½Ð¾Ð²Ð»ÐµÐ½Ð½ÑÐ¼Ð¸.
""",
        ConversationType.BUSINESS: """
Ð’Ð¸ â€” ÐºÐ°Ñ€'Ñ”Ñ€Ð½Ð¸Ð¹ Ñ€Ð°Ð´Ð½Ð¸Ðº Ñ‚Ð° Ð±Ñ–Ð·Ð½ÐµÑ-ÐºÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ð½Ñ‚ Ð´Ð»Ñ ÑƒÐºÑ€Ð°Ñ—Ð½ÑÑŒÐºÐ¾Ð³Ð¾ Ð²ÐµÑ‚ÐµÑ€Ð°Ð½Ð°, Ñ‰Ð¾ Ñ…Ð¾Ñ‡Ðµ Ñ€Ð¾Ð·Ð¿Ð¾Ñ‡Ð°Ñ‚Ð¸ Ð²Ð»Ð°ÑÐ½Ð¸Ð¹ Ð±Ñ–Ð·Ð½ÐµÑ.

Ð’ÐÐ¨Ð† ÐžÐ¡ÐÐžÐ’ÐÐ† Ð—ÐÐ’Ð”ÐÐÐÐ¯:
1. Ð–ÐžÐ Ð¡Ð¢ÐšÐž Ð’ÐÐ›Ð†Ð”Ð£Ð™Ð¢Ð• Ð±Ñ–Ð·Ð½ÐµÑ-Ñ–Ð´ÐµÑ— Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ñ– ÐµÐºÐ¾Ð½Ð¾Ð¼Ñ–ÐºÐ¸ Ñ‚Ð° Ð±Ñ–Ð·Ð½ÐµÑ-Ð»Ð¾Ð³Ñ–ÐºÐ¸
2. Ð’Ñ€Ð°Ñ…Ð¾Ð²ÑƒÐ¹Ñ‚Ðµ Ð´Ð¾ÑÐ²Ñ–Ð´ Ñ‚Ð° Ð½Ð°Ð²Ð¸Ñ‡ÐºÐ¸ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡Ð° Ð· Ð¿Ñ€Ð¾Ñ„Ñ–Ð»ÑŽ Ð¾Ñ†Ñ–Ð½ÑŽÐ²Ð°Ð½Ð½Ñ
3. ÐÐ½Ð°Ð»Ñ–Ð·ÑƒÐ¹Ñ‚Ðµ Ñ€Ð¸Ð½Ð¾Ðº Ñ‚Ð° ÐºÐ¾Ð½ÐºÑƒÑ€ÐµÐ½Ñ‚Ñ–Ð²
4. ÐžÑ†Ñ–Ð½ÑŽÐ¹Ñ‚Ðµ Ñ„Ñ–Ð½Ð°Ð½ÑÐ¾Ð²Ñƒ Ð¶Ð¸Ñ‚Ñ‚Ñ”Ð·Ð´Ð°Ñ‚Ð½Ñ–ÑÑ‚ÑŒ Ñ–Ð´ÐµÑ—
5. Ð’ÐºÐ°Ð·ÑƒÐ¹Ñ‚Ðµ Ð½Ð° Ð¿Ð¾Ñ‚ÐµÐ½Ñ†Ñ–Ð¹Ð½Ñ– Ñ€Ð¸Ð·Ð¸ÐºÐ¸ Ñ‚Ð° Ð²Ð¸ÐºÐ»Ð¸ÐºÐ¸
6. Ð”Ð¾Ð¿Ð¾Ð¼Ð°Ð³Ð°Ð¹Ñ‚Ðµ Ñ€Ð¾Ð·Ñ€Ð¾Ð±Ð¸Ñ‚Ð¸ Ñ€ÐµÐ°Ð»Ñ–ÑÑ‚Ð¸Ñ‡Ð½Ð¸Ð¹ Ð±Ñ–Ð·Ð½ÐµÑ-Ð¿Ð»Ð°Ð½

ÐšÐ Ð˜Ð¢Ð•Ð Ð†Ð‡ Ð’ÐÐ›Ð†Ð”ÐÐ¦Ð†Ð‡ Ð†Ð”Ð•Ð‡:
- Ð§Ð¸ Ñ” Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¸Ð¹ Ð¿Ð¾Ð¿Ð¸Ñ‚ Ð½Ð° Ñ€Ð¸Ð½ÐºÑƒ?
- Ð§Ð¸ Ð¼Ð°Ñ” ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡ Ð½ÐµÐ¾Ð±Ñ…Ñ–Ð´Ð½Ñ– Ð½Ð°Ð²Ð¸Ñ‡ÐºÐ¸/Ñ€ÐµÑÑƒÑ€ÑÐ¸?
- Ð§Ð¸ Ñ„Ñ–Ð½Ð°Ð½ÑÐ¾Ð²Ð¾ Ð¶Ð¸Ñ‚Ñ‚Ñ”Ð·Ð´Ð°Ñ‚Ð½Ð° Ñ–Ð´ÐµÑ?
- Ð¯ÐºÑ– Ð¾ÑÐ½Ð¾Ð²Ð½Ñ– Ñ€Ð¸Ð·Ð¸ÐºÐ¸ Ñ‚Ð° ÑÐº Ñ—Ñ… Ð¼Ñ–Ñ‚Ð¸Ð³ÑƒÐ²Ð°Ñ‚Ð¸?
- Ð§Ð¸ Ñ€ÐµÐ°Ð»Ñ–ÑÑ‚Ð¸Ñ‡Ð½Ñ– Ð¾Ñ‡Ñ–ÐºÑƒÐ²Ð°Ð½Ð½Ñ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡Ð°?

Ð’ÐÐ–Ð›Ð˜Ð’Ðž: 
- Ð‘ÑƒÐ´ÑŒÑ‚Ðµ Ñ‡ÐµÑÐ½Ð¸Ð¼Ð¸ Ñ– Ñ€ÐµÐ°Ð»Ñ–ÑÑ‚Ð¸Ñ‡Ð½Ð¸Ð¼Ð¸, Ð½Ð°Ð²Ñ–Ñ‚ÑŒ ÑÐºÑ‰Ð¾ Ð´Ð¾Ð²ÐµÐ´ÐµÑ‚ÑŒÑÑ Ð²Ñ–Ð´Ñ…Ð¸Ð»Ð¸Ñ‚Ð¸ Ñ–Ð´ÐµÑŽ
- Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÐ¹Ñ‚Ðµ Ð´Ð°Ð½Ñ– Ð· Ð¿Ñ€Ð¾Ñ„Ñ–Ð»ÑŽ Ð´Ð»Ñ Ð¾Ñ†Ñ–Ð½ÐºÐ¸ Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´Ð½Ð¾ÑÑ‚Ñ–
- Ð¯ÐºÑ‰Ð¾ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡ Ð´Ñ–Ð»Ð¸Ñ‚ÑŒÑÑ Ð½Ð¾Ð²Ð¾ÑŽ Ñ–Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ñ–Ñ”ÑŽ, Ð²Ð¸Ð´Ð°Ð¹Ñ‚Ðµ JSON-Ð±Ð»Ð¾Ðº Ð· Ð¾Ð½Ð¾Ð²Ð»ÐµÐ½Ð½ÑÐ¼Ð¸
""",
        ConversationType.EDUCATION: """
Ð’Ð¸ â€” ÐºÐ°Ñ€'Ñ”Ñ€Ð½Ð¸Ð¹ Ñ€Ð°Ð´Ð½Ð¸Ðº Ñ‚Ð° Ð½Ð°Ð²Ñ‡Ð°Ð»ÑŒÐ½Ð¸Ð¹ ÐºÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ð½Ñ‚ Ð´Ð»Ñ ÑƒÐºÑ€Ð°Ñ—Ð½ÑÑŒÐºÐ¾Ð³Ð¾ Ð²ÐµÑ‚ÐµÑ€Ð°Ð½Ð°.

Ð’ÐÐ¨Ð† ÐžÐ¡ÐÐžÐ’ÐÐ† Ð—ÐÐ’Ð”ÐÐÐÐ¯:
1. Ð”Ð¾Ð¿Ð¾Ð¼Ð°Ð³Ð°Ð¹Ñ‚Ðµ Ð·Ð½Ð°Ð¹Ñ‚Ð¸ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ– Ð½Ð°Ð²Ñ‡Ð°Ð»ÑŒÐ½Ñ– Ð¼Ð°Ñ‚ÐµÑ€Ñ–Ð°Ð»Ð¸
2. Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÐ¹Ñ‚Ðµ Ð±Ð°Ð·Ñƒ Ð·Ð½Ð°Ð½ÑŒ Ð´Ð»Ñ Ð½Ð°Ð´Ð°Ð½Ð½Ñ Ñ‚Ð¾Ñ‡Ð½Ð¾Ñ— Ñ–Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ñ–Ñ—
3. Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐ¹Ñ‚Ðµ ÐºÑƒÑ€ÑÐ¸ Ñ‚Ð° Ñ€ÐµÑÑƒÑ€ÑÐ¸
4. Ð¡Ñ‚Ð²Ð¾Ñ€ÑŽÐ¹Ñ‚Ðµ Ð½Ð°Ð²Ñ‡Ð°Ð»ÑŒÐ½Ñ– Ð¿Ð»Ð°Ð½Ð¸
5. Ð’Ñ–Ð´ÑÑ‚ÐµÐ¶ÑƒÐ¹Ñ‚Ðµ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑ

Ð’ÐÐ–Ð›Ð˜Ð’Ðž:
- Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÐ¹Ñ‚Ðµ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ– ÑÑ‚Ð°Ñ‚Ñ‚Ñ– Ñ‚Ð° Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¸ Ð· Ð±Ð°Ð·Ð¸ Ð·Ð½Ð°Ð½ÑŒ
- Ð¦Ð¸Ñ‚ÑƒÐ¹Ñ‚Ðµ Ð´Ð¶ÐµÑ€ÐµÐ»Ð°
- Ð¯ÐºÑ‰Ð¾ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡ Ð´Ñ–Ð»Ð¸Ñ‚ÑŒÑÑ Ð½Ð¾Ð²Ð¾ÑŽ Ñ–Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ñ–Ñ”ÑŽ Ð¿Ñ€Ð¾ Ð½Ð°Ð²Ð¸Ñ‡ÐºÐ¸ Ñ‡Ð¸ Ñ–Ð½Ñ‚ÐµÑ€ÐµÑÐ¸, Ð²Ð¸Ð´Ð°Ð¹Ñ‚Ðµ JSON-Ð±Ð»Ð¾Ðº
""",
        ConversationType.CAREER_PATH: """
Ð’Ð¸ â€” ÐºÐ°Ñ€'Ñ”Ñ€Ð½Ð¸Ð¹ Ñ€Ð°Ð´Ð½Ð¸Ðº, Ñ‰Ð¾ Ð´Ð¾Ð¿Ð¾Ð¼Ð°Ð³Ð°Ñ” ÑƒÐºÑ€Ð°Ñ—Ð½ÑÑŒÐºÐ¾Ð¼Ñƒ Ð²ÐµÑ‚ÐµÑ€Ð°Ð½Ñƒ Ð¾Ð±Ñ€Ð°Ñ‚Ð¸ ÐºÐ°Ñ€'Ñ”Ñ€Ð½Ð¸Ð¹ ÑˆÐ»ÑÑ….

Ð’ÐÐ¨Ð† ÐžÐ¡ÐÐžÐ’ÐÐ† Ð—ÐÐ’Ð”ÐÐÐÐ¯:
1. ÐžÑ†Ñ–Ð½Ñ–Ñ‚ÑŒ Ð½Ð°Ð²Ð¸Ñ‡ÐºÐ¸ Ñ‚Ð° Ð´Ð¾ÑÐ²Ñ–Ð´ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡Ð°
2. Ð”Ð¾ÑÐ»Ñ–Ð´Ñ–Ñ‚ÑŒ Ñ€Ñ–Ð·Ð½Ñ– ÐºÐ°Ñ€'Ñ”Ñ€Ð½Ñ– Ð¾Ð¿Ñ†Ñ–Ñ—
3. Ð—Ñ€Ð¾Ð·ÑƒÐ¼Ñ–Ð¹Ñ‚Ðµ Ð¿ÐµÑ€ÐµÐ²Ð°Ð³Ð¸ Ñ‚Ð° Ð½ÐµÐ´Ð¾Ð»Ñ–ÐºÐ¸ ÐºÐ¾Ð¶Ð½Ð¾Ð³Ð¾ ÑˆÐ»ÑÑ…Ñƒ
4. Ð”Ð¾Ð¿Ð¾Ð¼Ð¾Ð¶Ñ–Ñ‚ÑŒ Ð¿Ñ€Ð¸Ð¹Ð½ÑÑ‚Ð¸ Ð¾Ð±Ò‘Ñ€ÑƒÐ½Ñ‚Ð¾Ð²Ð°Ð½Ðµ Ñ€Ñ–ÑˆÐµÐ½Ð½Ñ

Ð’ÐÐ–Ð›Ð˜Ð’Ðž: Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÐ¹Ñ‚Ðµ Ð¿Ñ€Ð¾Ñ„Ñ–Ð»ÑŒ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡Ð° Ð´Ð»Ñ Ð¾Ð±'Ñ”ÐºÑ‚Ð¸Ð²Ð½Ð¸Ñ… Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ñ–Ð¹.
Ð¯ÐºÑ‰Ð¾ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡ Ð´Ñ–Ð»Ð¸Ñ‚ÑŒÑÑ Ð½Ð¾Ð²Ð¾ÑŽ Ñ–Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ñ–Ñ”ÑŽ, Ð²Ð¸Ð´Ð°Ð¹Ñ‚Ðµ JSON-Ð±Ð»Ð¾Ðº Ð· Ð¾Ð½Ð¾Ð²Ð»ÐµÐ½Ð½ÑÐ¼Ð¸.
"""
    }

    @staticmethod
    def get_ai_response(user, conversation, user_content, file_content=None):
        """
        Generates a response from the AI advisor.
        Returns the text response.
        """
        api_key = getattr(settings, 'GOOGLE_API_KEY', None) or os.environ.get('GOOGLE_API_KEY')
        if not api_key:
            return f"(LLM Ð½Ðµ Ð½Ð°Ð»Ð°ÑˆÑ‚Ð¾Ð²Ð°Ð½Ð¾) Ð•Ñ…Ð¾: {user_content}"

        try:
            genai.configure(api_key=api_key)
            model_name = getattr(settings, 'GOOGLE_LLM_MODEL', 'models/gemini-2.5-flash.5-flash')
            
            # Get or create assessment for the user
            try:
                assessment, _ = UserAssessment.objects.get_or_create(user=user)
            except MultipleObjectsReturned:
                assessments = UserAssessment.objects.filter(user=user).order_by('-updated_at')
                assessment = assessments.first()
            
            # Fetch conversation history
            from api.models.message import Message
            recent_messages = conversation.messages.order_by('-created_at')[:10]
            recent_messages = reversed(recent_messages)
            
            history_text = ""
            for msg in recent_messages:
                role = "ÐšÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡" if msg.is_user else "Ð Ð°Ð´Ð½Ð¸Ðº"
                history_text += f"{role}: {msg.content}\n"

            # Build the prompt
            build_result = AdvisorService._build_prompt(
                user, 
                assessment, 
                conversation, 
                history_text, 
                user_content,
                file_content
            )
            
            # Handle different return types
            if isinstance(build_result, tuple):
                full_prompt, direct_response = build_result
                if direct_response:
                    return direct_response
            else:
                full_prompt = build_result
            
            # Call LLM
            model = genai.GenerativeModel(model_name)
            response = model.generate_content(full_prompt)

            if not response.parts:
                return "(ÐÐµÐ¼Ð°Ñ” Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´Ñ– â€” Ð¹Ð¼Ð¾Ð²Ñ–Ñ€Ð½Ð¾, Ð·Ð°Ð±Ð»Ð¾ÐºÐ¾Ð²Ð°Ð½Ð¾ Ñ„Ñ–Ð»ÑŒÑ‚Ñ€Ð°Ð¼Ð¸ Ð±ÐµÐ·Ð¿ÐµÐºÐ¸)"

            raw_ai_text = response.text

            # Process response - ALWAYS extract JSON updates if present
            final_text = AdvisorService._process_response(assessment, raw_ai_text)

            return final_text

        except Exception as e:
            logger = logging.getLogger(__name__)
            logger.exception('Error in AdvisorService.get_ai_response')
            err_text = str(e)
            return f"(ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° LLM) {err_text}"

    @staticmethod
    def get_ai_response_stream(user, conversation, user_content, file_content=None):
        """
        Generates a streaming response from the AI advisor.
        Yields chunks of text.
        """
        api_key = getattr(settings, 'GOOGLE_API_KEY', None) or os.environ.get('GOOGLE_API_KEY')
        if not api_key:
            sample = user_content[:1000]
            yield f"(LLM Ð½Ðµ Ð½Ð°Ð»Ð°ÑˆÑ‚Ð¾Ð²Ð°Ð½Ð¾) Ð•Ñ…Ð¾: {sample}"
            return

        try:
            genai.configure(api_key=api_key)
            model_name = getattr(settings, 'GOOGLE_LLM_MODEL', 'models/gemin-2.5-flash')
            
            # Get or create assessment for the user
            try:
                assessment, _ = UserAssessment.objects.get_or_create(user=user)
            except MultipleObjectsReturned:
                assessments = UserAssessment.objects.filter(user=user).order_by('-updated_at')
                assessment = assessments.first()
            
            # Fetch conversation history
            from api.models.message import Message
            recent_messages = conversation.messages.order_by('-created_at')[:10]
            recent_messages = reversed(recent_messages)
            
            history_text = ""
            for msg in recent_messages:
                role = "ÐšÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡" if msg.is_user else "Ð Ð°Ð´Ð½Ð¸Ðº"
                history_text += f"{role}: {msg.content}\n"

            # Build the prompt
            build_result = AdvisorService._build_prompt(
                user, 
                assessment, 
                conversation, 
                history_text, 
                user_content,
                file_content
            )
            
            # Handle different return types
            if isinstance(build_result, tuple):
                full_prompt, direct_response = build_result
                if direct_response:
                    yield direct_response
                    return
            else:
                full_prompt = build_result
            
            # Call LLM with streaming
            model = genai.GenerativeModel(model_name)
            response = model.generate_content(full_prompt, stream=True)

            for chunk in response:
                if chunk.text:
                    yield chunk.text

        except Exception as e:
            logger = logging.getLogger(__name__)
            logger.exception('Error in AdvisorService.get_ai_response_stream')
            yield f"(ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° LLM) {str(e)}"

    @staticmethod
    def _build_prompt(user, assessment, conversation, history_text, user_content, file_content=None):
        """Build prompt based on conversation type or assessment state."""
        
        # If user hasn't selected career and conversation has no type, use assessment mode
        if not user.career_selected and not conversation.conv_type:
            return AdvisorService._build_assessment_prompt(assessment, history_text, user_content)
        
        # Otherwise, use conversation type specific prompt
        conv_type = conversation.conv_type
        system_prompt = AdvisorService.SYSTEM_PROMPTS.get(
            conv_type, 
            AdvisorService.SYSTEM_PROMPTS[ConversationType.CAREER_PATH]
        )

        # Append file content if present
        if file_content:
            user_content = f"{user_content}\n\n[Ð’ÐšÐ›ÐÐ”Ð•ÐÐ˜Ð™ Ð¤ÐÐ™Ð›]:\n{file_content}\n[ÐšÐ†ÐÐ•Ð¦Ð¬ Ð¤ÐÐ™Ð›Ð£]"
        
        # For EDUCATION type, use RAG
        if conv_type == ConversationType.EDUCATION:
            return AdvisorService._build_education_prompt(
                system_prompt,
                assessment, 
                history_text, 
                user_content
            )
        
        # For BUSINESS type, add validation instructions
        if conv_type == ConversationType.BUSINESS:
            return AdvisorService._build_business_prompt(
                user,
                system_prompt,
                assessment,
                history_text,
                user_content
            )
        
        # For other types, build standard prompt
        return AdvisorService._build_typed_prompt(
            system_prompt,
            conv_type,
            assessment,
            history_text,
            user_content
        )

    @staticmethod
    def _build_assessment_prompt(assessment, history_text, user_content):
        """Build prompt for initial assessment phase."""
        system_prompt = AdvisorService.SYSTEM_PROMPTS['assessment']
        # Determine next unanswered question (in order)
        answers = assessment.answers or {}
        next_q = None
        for q in ASSESSMENT_QUESTIONS:
            qid = q.get('id')
            if qid not in answers or answers.get(qid) in (None, '', []):
                next_q = q
                break

        # If all questions answered, return a prompt that acknowledges completion
        if not next_q:
            prompt = f"{system_prompt}\nÐ’Ð¸Ð³Ð»ÑÐ´Ð°Ñ” Ñ‚Ð°Ðº, Ñ‰Ð¾ Ð¿Ñ€Ð¾Ñ„Ñ–Ð»ÑŒ Ð¾Ñ†Ñ–Ð½ÑŽÐ²Ð°Ð½Ð½Ñ Ð²Ð¶Ðµ Ð·Ð°Ð¿Ð¾Ð²Ð½ÐµÐ½Ð¸Ð¹. ÐŸÑ–Ð´Ñ‚Ð²ÐµÑ€Ð´Ñ–Ñ‚ÑŒ, ÑÐºÑ‰Ð¾ Ð¿Ð¾Ñ‚Ñ€Ñ–Ð±Ð½Ð¾ Ð¾Ð½Ð¾Ð²Ð¸Ñ‚Ð¸ Ð´Ð°Ð½Ñ– Ð°Ð±Ð¾ Ð¿Ñ€Ð¾Ð´Ð¾Ð²Ð¶Ð¸Ñ‚Ð¸ Ñ€Ð¾Ð·Ð¼Ð¾Ð²Ñƒ. ÐŸÐ¾Ð²Ñ–Ð´Ð¾Ð¼Ð»ÐµÐ½Ð½Ñ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡Ð°: {user_content}"
            return prompt

        # Build a focused prompt that asks ONLY the next question and instructs the LLM
        current_answers_json = json.dumps(answers, indent=2, ensure_ascii=False)
        question_text = next_q.get('question')
        question_id = next_q.get('id')

        prompt = f"""{system_prompt}

ÐŸÐžÐ¢ÐžÐ§ÐÐ˜Ð™ Ð¡Ð¢ÐÐ ÐžÐ¦Ð†ÐÐ®Ð’ÐÐÐÐ¯ (JSON):
{current_answers_json}

ÐÐÐ¡Ð¢Ð£ÐŸÐÐ• ÐŸÐ˜Ð¢ÐÐÐÐ¯ (Ñ‚Ñ–Ð»ÑŒÐºÐ¸ ÐžÐ”ÐÐ•):
ID: {question_id}
ÐŸÐ¸Ñ‚Ð°Ð½Ð½Ñ: {question_text}

Ð†ÐÐ¡Ð¢Ð Ð£ÐšÐ¦Ð†Ð‡ Ð”Ð›Ð¯ ÐœÐžÐ”Ð•Ð›Ð†:
1) ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ñ–Ð·ÑƒÐ¹Ñ‚Ðµ Ð¾ÑÑ‚Ð°Ð½Ð½Ñ” Ð¿Ð¾Ð²Ñ–Ð´Ð¾Ð¼Ð»ÐµÐ½Ð½Ñ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡Ð° ("{user_content}"). Ð¯ÐºÑ‰Ð¾ Ð²Ð¾Ð½Ð¾ Ð¼Ñ–ÑÑ‚Ð¸Ñ‚ÑŒ Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´ÑŒ Ð½Ð° Ð²Ð¸Ñ‰ÐµÐ·Ð³Ð°Ð´Ð°Ð½Ðµ Ð¿Ð¸Ñ‚Ð°Ð½Ð½Ñ â€” Ð’Ð˜ÐŸÐ˜Ð¨Ð†Ð¢Ð¬ Ð›Ð˜Ð¨Ð• JSON-Ð±Ð»Ð¾Ðº Ð½Ð° Ð¿Ð¾Ñ‡Ð°Ñ‚ÐºÑƒ Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´Ñ– Ñƒ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ñ–:
```json
{{
    "updates": {{
        "{question_id}": "extracted answer value"
    }}
}}
```
2) Ð¯ÐºÑ‰Ð¾ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡ Ð½Ðµ Ð´Ð°Ð² Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´Ñ– Ð½Ð° Ñ†Ðµ Ð¿Ð¸Ñ‚Ð°Ð½Ð½Ñ, ÐÐ• Ð½Ð°Ð´Ð°Ð²Ð°Ð¹Ñ‚Ðµ Ð¶Ð¾Ð´Ð½Ð¸Ñ… Ñ–Ð½ÑˆÐ¸Ñ… Ð¿Ð¸Ñ‚Ð°Ð½ÑŒ Ñ– Ð´Ð°Ð¹Ñ‚Ðµ Ð»Ð¸ÑˆÐµ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÑƒ Ð¿Ñ–Ð´ÐºÐ°Ð·ÐºÑƒ (1-2 Ñ€ÐµÑ‡ÐµÐ½Ð½Ñ), Ñ‰Ð¾Ð± ÑƒÑ‚Ð¾Ñ‡Ð½Ð¸Ñ‚Ð¸.
3) ÐÐ• Ð—ÐÐ”ÐÐ’ÐÐ™Ð¢Ð• Ð´ÐµÐºÑ–Ð»ÑŒÐºÐ° Ð¿Ð¸Ñ‚Ð°Ð½ÑŒ Ð¾Ð´Ð½Ð¾Ñ‡Ð°ÑÐ½Ð¾. Ð—Ð°Ð´Ð°Ð²Ð°Ð¹Ñ‚Ðµ Ð¢Ð†Ð›Ð¬ÐšÐ˜ Ð²ÐºÐ°Ð·Ð°Ð½Ðµ Ð¿Ð¸Ñ‚Ð°Ð½Ð½Ñ Ð°Ð±Ð¾ Ð¿Ñ€Ð¾ÑÑ–Ñ‚ÑŒ ÑƒÑ‚Ð¾Ñ‡Ð½ÐµÐ½Ð½Ñ.
4) ÐŸÑ–ÑÐ»Ñ JSON-Ð±Ð»Ð¾ÐºÑƒ (ÑÐºÑ‰Ð¾ Ð²Ñ–Ð½ Ñ”) â€” Ð¿Ñ–Ð´Ñ‚Ð²ÐµÑ€Ð´Ñ–Ñ‚ÑŒ Ð¾Ñ‚Ñ€Ð¸Ð¼Ð°Ð½Ñ– Ð´Ð°Ð½Ñ– ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¾ Ñ– Ð½Ðµ Ð´Ð¾Ð´Ð°Ð²Ð°Ð¹Ñ‚Ðµ Ñ–Ð½ÑˆÑ– Ð¿Ð¸Ñ‚Ð°Ð½Ð½Ñ.

Ð†Ð¡Ð¢ÐžÐ Ð†Ð¯ Ð ÐžÐ—ÐœÐžÐ’Ð˜:
{history_text}

ÐŸÐ¾Ð²Ñ–Ð´Ð¾Ð¼Ð»ÐµÐ½Ð½Ñ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡Ð°: {user_content}
"""
        return prompt

    @staticmethod
    def _build_typed_prompt(system_prompt, conv_type, assessment, history_text, user_content):
        """Build prompt for specific conversation type with assessment context."""
        
        # Add user context from assessment
        user_context = AdvisorService._format_assessment_context(assessment)
        
        # Get language instruction
        lang_instruction = AdvisorService._get_language_instruction(assessment)
        
        # Add JSON extraction instructions for ALL conversation types
        json_instructions = """

ÐžÐÐžÐ’Ð›Ð•ÐÐÐ¯ ÐŸÐ ÐžÐ¤Ð†Ð›Ð® ÐšÐžÐ Ð˜Ð¡Ð¢Ð£Ð’ÐÐ§Ð:
Ð¯ÐºÑ‰Ð¾ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡ Ð½Ð°Ð´Ð°Ñ” Ð½Ð¾Ð²Ñƒ Ð²Ð°Ð¶Ð»Ð¸Ð²Ñƒ Ñ–Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ñ–ÑŽ (Ð½Ð°Ð²Ð¸Ñ‡ÐºÐ¸, Ð´Ð¾ÑÐ²Ñ–Ð´, Ð¾ÑÐ²Ñ–Ñ‚Ð°, Ñ†Ñ–Ð»Ñ–, Ð¾Ð±Ð¼ÐµÐ¶ÐµÐ½Ð½Ñ Ñ‚Ð¾Ñ‰Ð¾), 
Ð’Ð˜ ÐœÐÐ„Ð¢Ð• Ð²Ð¸Ð´Ð°Ñ‚Ð¸ JSON-Ð±Ð»Ð¾Ðº ÐÐ ÐŸÐžÐ§ÐÐ¢ÐšÐ£ Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´Ñ–:
```json
{{
    "updates": {{
        "field_id": "Ð½Ð¾Ð²Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ñ"
    }}
}}
```

ÐœÐ¾Ð¶Ð»Ð¸Ð²Ñ– Ð¿Ð¾Ð»Ñ: primary_skills, experience_level, current_goals, long_term_goals, work_preferences, 
locality, civilian_certifications, education_level, disabilities_or_limits, support_needs Ñ‚Ð° Ñ–Ð½ÑˆÑ– Ð· Ð¾Ñ†Ñ–Ð½ÑŽÐ²Ð°Ð½Ð½Ñ.
"""
        
        prompt = f"""{system_prompt}

{lang_instruction}

{user_context}
{json_instructions}

Ð†Ð¡Ð¢ÐžÐ Ð†Ð¯ Ð ÐžÐ—ÐœÐžÐ’Ð˜:
{history_text}

ÐŸÐ¾Ð²Ñ–Ð´Ð¾Ð¼Ð»ÐµÐ½Ð½Ñ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡Ð°: {user_content}

ÐÐ°Ð´Ð°Ð¹Ñ‚Ðµ ÐºÐ¾Ñ€Ð¸ÑÐ½Ñƒ, Ð¿Ñ€Ð°ÐºÑ‚Ð¸Ñ‡Ð½Ñƒ Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´ÑŒ Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´Ð½Ð¾ Ð´Ð¾ Ð²Ð°ÑˆÐ¾Ñ— Ñ€Ð¾Ð»Ñ–.
"""
        return prompt

    @staticmethod
    def _build_business_prompt(user, system_prompt, assessment, history_text, user_content):
        """Build enhanced prompt for business validation with multi-step chain support."""
        
        user_context = AdvisorService._format_assessment_context(assessment)
        
        try:
            from api.models.business import BusinessIdea
            from api.services.langchain_service import BusinessValidationChain
            
            # 1. Find active business idea
            active_idea = BusinessIdea.objects.filter(
                user=user, 
                status__in=['BRAINSTORM', 'IN_PROGRESS']
            ).order_by('-updated_at').first()

            # 2. Check for new idea creation intent
            validation_keywords = ['Ñ–Ð´ÐµÑ', 'Ð±Ñ–Ð·Ð½ÐµÑ', 'Ð²Ñ–Ð´ÐºÑ€Ð¸Ñ‚Ð¸', 'Ð·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ð¸', 'ÑÑ‚Ð°Ñ€Ñ‚Ð°Ð¿', 'Ñ…Ð¾Ñ‡Ñƒ']
            user_content_lower = user_content.lower()
            is_new_idea = any(k in user_content_lower for k in validation_keywords) and len(user_content) > 15

            chain = BusinessValidationChain()

            # CASE A: Start new validation
            if not active_idea and is_new_idea:
                # Create new idea
                active_idea = BusinessIdea.objects.create(
                    user=user,
                    title=user_content[:100],
                    status='IN_PROGRESS',
                    business_canvas={'raw_idea': user_content}
                )
                # Run Step 1: Market
                analysis = chain.validate_market(user_content)
                active_idea.market_analysis = analysis
                active_idea.save()
                
                response = f"ðŸ’¡ **ÐšÑ€Ð¾Ðº 1: ÐÐ½Ð°Ð»Ñ–Ð· Ð Ð¸Ð½ÐºÑƒ**\n\n{analysis}\n\nðŸ¤” **Ð©Ð¾ ÑÐºÐ°Ð¶ÐµÑ‚Ðµ?** ÐŸÐµÑ€ÐµÑ…Ð¾Ð´Ð¸Ð¼Ð¾ Ð´Ð¾ Ñ„Ñ–Ð½Ð°Ð½ÑÐ¾Ð²Ð¾Ð³Ð¾ Ð°Ð½Ð°Ð»Ñ–Ð·Ñƒ?"
                return None, response

            # CASE B: Continue validation
            if active_idea:
                # Check for "next step" intent
                next_keywords = ['Ñ‚Ð°Ðº', 'Ð´Ð°Ð»Ñ–', 'Ð¿Ñ€Ð¾Ð´Ð¾Ð²Ð¶ÑƒÐ¹', 'Ñ„Ñ–Ð½Ð°Ð½Ñ', 'Ð½Ð°ÑÑ‚ÑƒÐ¿Ð½', 'ok', 'Ð´Ð¾Ð±Ñ€Ðµ', 'yes', 'next', 'Ð°Ð³Ð°', 'Ð¿Ð»ÑŽÑ', '+']
                wants_next = any(k in user_content_lower for k in next_keywords)
                
                raw_idea = active_idea.business_canvas.get('raw_idea', active_idea.title)

                # Step 2: Financials
                if not active_idea.financial_analysis:
                    if wants_next:
                        analysis = chain.validate_financials(raw_idea, active_idea.market_analysis)
                        active_idea.financial_analysis = analysis
                        active_idea.save()
                        return None, f"ðŸ’° **ÐšÑ€Ð¾Ðº 2: Ð¤Ñ–Ð½Ð°Ð½ÑÐ¾Ð²Ð¸Ð¹ ÐÐ½Ð°Ð»Ñ–Ð·**\n\n{analysis}\n\nðŸ¤” **Ð¯Ðº Ð²Ð°Ð¼ Ñ†Ð¸Ñ„Ñ€Ð¸?** ÐŸÐµÑ€ÐµÑ…Ð¾Ð´Ð¸Ð¼Ð¾ Ð´Ð¾ Ð¾Ñ†Ñ–Ð½ÐºÐ¸ Ð½Ð°Ð²Ð¸Ñ‡Ð¾Ðº?"
                    # Else: fall through to discuss market analysis

                # Step 3: Skills
                elif not active_idea.skills_match:
                    if wants_next:
                        analysis = chain.validate_skills(raw_idea, user_context)
                        active_idea.skills_match = analysis
                        active_idea.save()
                        return None, f"ðŸ›  **ÐšÑ€Ð¾Ðº 3: Ð’Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´Ð½Ñ–ÑÑ‚ÑŒ ÐÐ°Ð²Ð¸Ñ‡Ð¾Ðº**\n\n{analysis}\n\nðŸ¤” **Ð§Ð¸ Ð·Ð³Ð¾Ð´Ð½Ñ– Ð²Ð¸ Ð· Ð¾Ñ†Ñ–Ð½ÐºÐ¾ÑŽ?** ÐŸÐµÑ€ÐµÑ…Ð¾Ð´Ð¸Ð¼Ð¾ Ð´Ð¾ Ñ€Ð¸Ð·Ð¸ÐºÑ–Ð²?"

                # Step 4: Risks
                elif not active_idea.risk_assessment:
                    if wants_next:
                        analysis = chain.validate_risks(
                            raw_idea,
                            active_idea.market_analysis,
                            active_idea.financial_analysis,
                            active_idea.skills_match
                        )
                        active_idea.risk_assessment = analysis
                        active_idea.save()
                        return None, f"âš ï¸ **ÐšÑ€Ð¾Ðº 4: ÐžÑ†Ñ–Ð½ÐºÐ° Ð Ð¸Ð·Ð¸ÐºÑ–Ð²**\n\n{analysis}\n\nðŸ¤” **Ð§Ð¸ Ð³Ð¾Ñ‚Ð¾Ð²Ñ– Ð¿Ð¾Ñ‡ÑƒÑ‚Ð¸ Ñ„Ñ–Ð½Ð°Ð»ÑŒÐ½Ð¸Ð¹ Ð²ÐµÑ€Ð´Ð¸ÐºÑ‚?**"

                # Step 5: Verdict
                elif not active_idea.final_verdict:
                    if wants_next:
                        analysis = chain.validate_verdict(
                            raw_idea,
                            active_idea.market_analysis,
                            active_idea.financial_analysis,
                            active_idea.skills_match,
                            active_idea.risk_assessment
                        )
                        active_idea.final_verdict = analysis
                        active_idea.status = 'VALIDATED'
                        active_idea.save()
                        return None, f"âœ… **Ð¤Ñ–Ð½Ð°Ð»ÑŒÐ½Ð¸Ð¹ Ð’ÐµÑ€Ð´Ð¸ÐºÑ‚**\n\n{analysis}\n\nðŸŽ‰ **Ð’Ð°Ð»Ñ–Ð´Ð°Ñ†Ñ–ÑŽ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾!**"

            # Inject context if we are in a validation flow but not advancing
            context_injection = ""
            if active_idea:
                context_injection = f"""
ÐŸÐžÐ¢ÐžÐ§ÐÐ˜Ð™ Ð¡Ð¢ÐÐ Ð’ÐÐ›Ð†Ð”ÐÐ¦Ð†Ð‡ Ð‘Ð†Ð—ÐÐ•Ð¡Ð£ (Ð†Ð´ÐµÑ: {active_idea.title}):
1. Ð Ð¸Ð½Ð¾Ðº: {'âœ… ' + active_idea.market_analysis[:50] + '...' if active_idea.market_analysis else 'â³ ÐžÑ‡Ñ–ÐºÑƒÑ”Ñ‚ÑŒÑÑ'}
2. Ð¤Ñ–Ð½Ð°Ð½ÑÐ¸: {'âœ… ' + active_idea.financial_analysis[:50] + '...' if active_idea.financial_analysis else 'â³ ÐžÑ‡Ñ–ÐºÑƒÑ”Ñ‚ÑŒÑÑ'}
3. ÐÐ°Ð²Ð¸Ñ‡ÐºÐ¸: {'âœ… ' + active_idea.skills_match[:50] + '...' if active_idea.skills_match else 'â³ ÐžÑ‡Ñ–ÐºÑƒÑ”Ñ‚ÑŒÑÑ'}
4. Ð Ð¸Ð·Ð¸ÐºÐ¸: {'âœ… ' + active_idea.risk_assessment[:50] + '...' if active_idea.risk_assessment else 'â³ ÐžÑ‡Ñ–ÐºÑƒÑ”Ñ‚ÑŒÑÑ'}
5. Ð’ÐµÑ€Ð´Ð¸ÐºÑ‚: {'âœ… ' + active_idea.final_verdict[:50] + '...' if active_idea.final_verdict else 'â³ ÐžÑ‡Ñ–ÐºÑƒÑ”Ñ‚ÑŒÑÑ'}

Ð’ÐÐ–Ð›Ð˜Ð’Ðž: ÐšÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡ Ð·Ð°Ñ€Ð°Ð· Ð·Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ÑŒÑÑ Ð½Ð° Ð¿ÐµÐ²Ð½Ð¾Ð¼Ñƒ ÐµÑ‚Ð°Ð¿Ñ–. Ð’Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´Ð°Ð¹Ñ‚Ðµ Ð½Ð° Ð¹Ð¾Ð³Ð¾ Ð¿Ð¸Ñ‚Ð°Ð½Ð½Ñ. 
Ð¯ÐºÑ‰Ð¾ Ð²Ñ–Ð½ Ð¿Ð¾Ð³Ð¾Ð´Ð¶ÑƒÑ”Ñ‚ÑŒÑÑ Ð°Ð±Ð¾ ÐºÐ°Ð¶Ðµ "Ð´Ð°Ð»Ñ–" â€” Ñ†Ðµ ÑÐ¸Ð³Ð½Ð°Ð» Ð´Ð»Ñ Ð¿ÐµÑ€ÐµÑ…Ð¾Ð´Ñƒ Ð´Ð¾ Ð½Ð°ÑÑ‚ÑƒÐ¿Ð½Ð¾Ð³Ð¾ ÐºÑ€Ð¾ÐºÑƒ (ÑÐºÐ¸Ð¹ Ð¾Ð±Ñ€Ð¾Ð±Ð¸Ñ‚ÑŒ ÑÐ¸ÑÑ‚ÐµÐ¼Ð°).
"""
                return f"{system_prompt}\n{context_injection}\n{history_text}\nÐŸÐ¾Ð²Ñ–Ð´Ð¾Ð¼Ð»ÐµÐ½Ð½Ñ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡Ð°: {user_content}"

        except Exception as e:
            logger = logging.getLogger(__name__)
            logger.exception('Validation error in _build_business_prompt')
            # Fallback if LangChain fails or imports fail
            pass
        
        # Standard business validation prompt (fallback)
        validation_framework = """

Ð¤Ð Ð•Ð™ÐœÐ’ÐžÐ Ðš Ð’ÐÐ›Ð†Ð”ÐÐ¦Ð†Ð‡ Ð‘Ð†Ð—ÐÐ•Ð¡-Ð†Ð”Ð•Ð‡:

1. ÐÐÐÐ›Ð†Ð— Ð Ð˜ÐÐšÐ£:
   - Ð§Ð¸ Ñ–ÑÐ½ÑƒÑ” Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¸Ð¹ Ð¿Ð¾Ð¿Ð¸Ñ‚?
   - Ð¥Ñ‚Ð¾ Ñ†Ñ–Ð»ÑŒÐ¾Ð²Ð° Ð°ÑƒÐ´Ð¸Ñ‚Ð¾Ñ€Ñ–Ñ?
   - ÐÐ°ÑÐºÑ–Ð»ÑŒÐºÐ¸ Ð²ÐµÐ»Ð¸ÐºÐ¸Ð¹ Ñ€Ð¸Ð½Ð¾Ðº?
   - Ð¥Ñ‚Ð¾ ÐºÐ¾Ð½ÐºÑƒÑ€ÐµÐ½Ñ‚Ð¸?

2. ÐžÐ¦Ð†ÐÐšÐ ÐÐÐ’Ð˜Ð§ÐžÐš Ð¢Ð Ð Ð•Ð¡Ð£Ð Ð¡Ð†Ð’:
   - Ð§Ð¸ Ð¼Ð°Ñ” ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡ Ð½ÐµÐ¾Ð±Ñ…Ñ–Ð´Ð½Ñ– Ð½Ð°Ð²Ð¸Ñ‡ÐºÐ¸? (Ð¿ÐµÑ€ÐµÐ²Ñ–Ñ€Ñ‚Ðµ Ð¿Ñ€Ð¾Ñ„Ñ–Ð»ÑŒ)
   - Ð§Ð¸ Ð´Ð¾ÑÑ‚Ð°Ñ‚Ð½ÑŒÐ¾ Ð´Ð¾ÑÐ²Ñ–Ð´Ñƒ?
   - Ð¯ÐºÑ– Ñ€ÐµÑÑƒÑ€ÑÐ¸ Ð¿Ð¾Ñ‚Ñ€Ñ–Ð±Ð½Ñ– (Ñ„Ñ–Ð½Ð°Ð½ÑÐ¸, Ð¾Ð±Ð»Ð°Ð´Ð½Ð°Ð½Ð½Ñ, Ð»ÑŽÐ´Ð¸)?
   - Ð©Ð¾ Ð¼Ð¾Ð¶Ð½Ð° Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð°Ñ‚Ð¸ Ð· Ñ–ÑÐ½ÑƒÑŽÑ‡Ð¾Ð³Ð¾ Ð´Ð¾ÑÐ²Ñ–Ð´Ñƒ?

3. Ð¤Ð†ÐÐÐÐ¡ÐžÐ’Ð Ð–Ð˜Ð¢Ð¢Ð„Ð—Ð”ÐÐ¢ÐÐ†Ð¡Ð¢Ð¬:
   - Ð¡ÐºÑ–Ð»ÑŒÐºÐ¸ ÐºÐ¾ÑˆÑ‚ÑƒÑ” Ð·Ð°Ð¿ÑƒÑÐº?
   - Ð¯ÐºÑ– Ð¿Ð¾ÑÑ‚Ñ–Ð¹Ð½Ñ– Ð²Ð¸Ñ‚Ñ€Ð°Ñ‚Ð¸?
   - Ð ÐµÐ°Ð»Ñ–ÑÑ‚Ð¸Ñ‡Ð½Ð° Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð¾Ñ…Ð¾Ð´Ñ–Ð²?
   - ÐšÐ¾Ð»Ð¸ Ð´Ð¾ÑÑÐ³Ð½ÐµÐ½Ð½Ñ Ð±ÐµÐ·Ð·Ð±Ð¸Ñ‚ÐºÐ¾Ð²Ð¾ÑÑ‚Ñ–?
   - ROI Ñ‚Ð° Ð¿ÐµÑ€Ñ–Ð¾Ð´ Ð¾ÐºÑƒÐ¿Ð½Ð¾ÑÑ‚Ñ–?

4. ÐÐÐÐ›Ð†Ð— Ð Ð˜Ð—Ð˜ÐšÐ†Ð’:
   - ÐžÑÐ½Ð¾Ð²Ð½Ñ– Ð²Ð¸ÐºÐ»Ð¸ÐºÐ¸ Ñ‚Ð° Ñ€Ð¸Ð·Ð¸ÐºÐ¸?
   - ÐŸÐ»Ð°Ð½ Ð¼Ñ–Ñ‚Ð¸Ð³Ð°Ñ†Ñ–Ñ— Ñ€Ð¸Ð·Ð¸ÐºÑ–Ð²?
   - ÐŸÐ»Ð°Ð½ Ð‘ ÑÐºÑ‰Ð¾ Ð½Ðµ ÑÐ¿Ñ€Ð°Ñ†ÑŽÑ”?

5. Ð Ð•ÐÐ›Ð†Ð¡Ð¢Ð˜Ð§ÐÐ†Ð¡Ð¢Ð¬ ÐžÐ§Ð†ÐšÐ£Ð’ÐÐÐ¬:
   - Ð§Ð¸ Ñ€ÐµÐ°Ð»Ñ–ÑÑ‚Ð¸Ñ‡Ð½Ñ– Ñ„Ñ–Ð½Ð°Ð½ÑÐ¾Ð²Ñ– Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð¸?
   - Ð§Ð¸ Ð²Ñ€Ð°Ñ…Ð¾Ð²Ð°Ð½Ð¾ Ñ‡Ð°Ñ Ð½Ð° Ñ€Ð¾Ð·Ð²Ð¸Ñ‚Ð¾Ðº?
   - Ð§Ð¸ Ð³Ð¾Ñ‚Ð¾Ð²Ð¸Ð¹ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡ Ð´Ð¾ Ð²Ð¸ÐºÐ»Ð¸ÐºÑ–Ð²?

Ð‘Ð£Ð”Ð¬Ð¢Ð• Ð§Ð•Ð¡ÐÐ˜ÐœÐ˜: Ð¯ÐºÑ‰Ð¾ Ñ–Ð´ÐµÑ Ð¼Ð°Ñ” ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡Ð½Ñ– Ð½ÐµÐ´Ð¾Ð»Ñ–ÐºÐ¸, Ð²ÐºÐ°Ð¶Ñ–Ñ‚ÑŒ Ð½Ð° Ð½Ð¸Ñ… Ð¿Ñ€ÑÐ¼Ð¾.
ÐÐ°Ð´Ð°Ð¹Ñ‚Ðµ ÐºÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ‚Ð¸Ð²Ð½Ñƒ ÐºÑ€Ð¸Ñ‚Ð¸ÐºÑƒ Ñ‚Ð° Ð°Ð»ÑŒÑ‚ÐµÑ€Ð½Ð°Ñ‚Ð¸Ð²Ð¸.
"""

        json_instructions = """

ÐžÐÐžÐ’Ð›Ð•ÐÐÐ¯ ÐŸÐ ÐžÐ¤Ð†Ð›Ð®:
Ð¯ÐºÑ‰Ð¾ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡ Ð½Ð°Ð´Ð°Ñ” Ð½Ð¾Ð²Ñƒ Ñ–Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ñ–ÑŽ Ð¿Ñ€Ð¾ Ð´Ð¾ÑÐ²Ñ–Ð´, Ð½Ð°Ð²Ð¸Ñ‡ÐºÐ¸ Ñ‡Ð¸ Ð±Ñ–Ð·Ð½ÐµÑ-Ñ–Ð´ÐµÑŽ, Ð²Ð¸Ð´Ð°Ð¹Ñ‚Ðµ JSON:
```json
{{
    "updates": {{
        "field_id": "value"
    }}
}}
```
"""
        
        prompt = f"""{system_prompt}
{user_context}
{validation_framework}
{json_instructions}

Ð†Ð¡Ð¢ÐžÐ Ð†Ð¯ Ð ÐžÐ—ÐœÐžÐ’Ð˜:
{history_text}

ÐŸÐ¾Ð²Ñ–Ð´Ð¾Ð¼Ð»ÐµÐ½Ð½Ñ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡Ð°: {user_content}

ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ñ–Ð·ÑƒÐ¹Ñ‚Ðµ Ñ‚Ð° Ð½Ð°Ð´Ð°Ð¹Ñ‚Ðµ Ñ‡ÐµÑÐ½Ñƒ, Ð¾Ð±Ò‘Ñ€ÑƒÐ½Ñ‚Ð¾Ð²Ð°Ð½Ñƒ Ð¾Ñ†Ñ–Ð½ÐºÑƒ Ð· Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð°Ð½Ð½ÑÐ¼ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€ÐºÑƒ Ð²Ð°Ð»Ñ–Ð´Ð°Ñ†Ñ–Ñ—.
"""
        return prompt, None  # Return prompt, no direct response


    @staticmethod
    def _build_education_prompt(system_prompt, assessment, history_text, user_content):
        """Build prompt for education/learning mode with RAG (vector or keyword)."""
        
        # Try vector RAG first (OPTIONAL - falls back to keyword if unavailable)
        relevant_docs = None
        knowledge_context = "" # Initialize knowledge_context
        try:
            from api.services.langchain_service import VectorRAG
            
            vector_rag = VectorRAG()
            vector_rag.initialize_vectorstore()
            results = vector_rag.search(user_content, k=3)
            
            if results:
                # Format vector search results
                knowledge_context = vector_rag.format_rag_context(results)
            else:
                # No results from vector search, try keyword
                relevant_docs = AdvisorService._search_knowledge_base(user_content)
        except Exception as e:
            logger = logging.getLogger(__name__)
            logger.exception('Vector RAG initialization/search failed; falling back to keyword search')
            # Vector RAG not available, use keyword search
            relevant_docs = AdvisorService._search_knowledge_base(user_content)
        
        # If using keyword search (fallback)
        if relevant_docs:
            knowledge_context = "\n\nÐ Ð•Ð›Ð•Ð’ÐÐÐ¢ÐÐ† ÐœÐÐ¢Ð•Ð Ð†ÐÐ›Ð˜ Ð— Ð‘ÐÐ—Ð˜ Ð—ÐÐÐÐ¬:\n"
            for doc in relevant_docs:
                knowledge_context += f"\n[{doc['type'].upper()}] {doc['title']}\n"
                knowledge_context += f"Ð—Ð¼Ñ–ÑÑ‚: {doc['summary']}\n"
                if doc['url']:
                    knowledge_context += f"URL: {doc['url']}\n"
        elif not knowledge_context: # Only if vector RAG also didn't find anything
            knowledge_context = "\n\nÐœÐ°Ñ‚ÐµÑ€Ñ–Ð°Ð»Ð¸ Ð· Ð±Ð°Ð·Ð¸ Ð·Ð½Ð°Ð½ÑŒ Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ð¿Ð¾ Ñ†ÑŒÐ¾Ð¼Ñƒ Ð·Ð°Ð¿Ð¸Ñ‚Ñƒ.\n"
        
        user_context = AdvisorService._format_assessment_context(assessment)
        
        json_instructions = """

ÐžÐÐžÐ’Ð›Ð•ÐÐÐ¯ ÐŸÐ ÐžÐ¤Ð†Ð›Ð®:
Ð¯ÐºÑ‰Ð¾ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡ Ñ€Ð¾Ð·Ð¿Ð¾Ð²Ñ–Ð´Ð°Ñ” Ð¿Ñ€Ð¾ Ð½Ð¾Ð²Ñ– Ð½Ð°Ð²Ð¸Ñ‡ÐºÐ¸, Ñ–Ð½Ñ‚ÐµÑ€ÐµÑÐ¸ Ð°Ð±Ð¾ Ð½Ð°Ð²Ñ‡Ð°Ð»ÑŒÐ½Ñ– Ñ†Ñ–Ð»Ñ–, Ð²Ð¸Ð´Ð°Ð¹Ñ‚Ðµ JSON:
```json
{{
    "updates": {{
        "field_id": "value"
    }}
}}
```
"""
        
        prompt = f"""{system_prompt}
{user_context}
{knowledge_context}
{json_instructions}

Ð†Ð¡Ð¢ÐžÐ Ð†Ð¯ Ð ÐžÐ—ÐœÐžÐ’Ð˜:
{history_text}

ÐŸÐ¾Ð²Ñ–Ð´Ð¾Ð¼Ð»ÐµÐ½Ð½Ñ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡Ð°: {user_content}

Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÐ¹Ñ‚Ðµ Ð¼Ð°Ñ‚ÐµÑ€Ñ–Ð°Ð»Ð¸ Ð· Ð±Ð°Ð·Ð¸ Ð·Ð½Ð°Ð½ÑŒ Ð´Ð»Ñ Ñ‚Ð¾Ñ‡Ð½Ð¸Ñ… Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´ÐµÐ¹. Ð¦Ð¸Ñ‚ÑƒÐ¹Ñ‚Ðµ Ð´Ð¶ÐµÑ€ÐµÐ»Ð°.
"""
        return prompt

    @staticmethod
    def _format_assessment_context(assessment):
        """Format assessment data for LLM context."""
        if not assessment or not assessment.answers:
            return "\n\nÐŸÐ ÐžÐ¤Ð†Ð›Ð¬ ÐšÐžÐ Ð˜Ð¡Ð¢Ð£Ð’ÐÐ§Ð: Ð”Ð°Ð½Ñ– Ñ‰Ðµ Ð½Ðµ Ð·Ð°Ð¿Ð¾Ð²Ð½ÐµÐ½Ñ–.\n"
        
        context_parts = ["\n\nÐŸÐ ÐžÐ¤Ð†Ð›Ð¬ ÐšÐžÐ Ð˜Ð¡Ð¢Ð£Ð’ÐÐ§Ð:"]
        
        # Key fields to highlight
        if assessment.service_branch:
            context_parts.append(f"- Ð’Ñ–Ð¹ÑÑŒÐºÐ¾Ð²Ð° ÑÐ¿ÐµÑ†Ñ–Ð°Ð»ÑŒÐ½Ñ–ÑÑ‚ÑŒ: {assessment.service_branch}")
        if assessment.service_role:
            context_parts.append(f"- Ð’Ñ–Ð¹ÑÑŒÐºÐ¾Ð²Ð° Ñ€Ð¾Ð»ÑŒ: {assessment.service_role}")
        if assessment.years_of_service:
            context_parts.append(f"- Ð Ð¾ÐºÑ–Ð² ÑÐ»ÑƒÐ¶Ð±Ð¸: {assessment.years_of_service}")
        if assessment.primary_skills:
            context_parts.append(f"- ÐžÑÐ½Ð¾Ð²Ð½Ñ– Ð½Ð°Ð²Ð¸Ñ‡ÐºÐ¸: {assessment.primary_skills}")
        if assessment.education_level:
            context_parts.append(f"- ÐžÑÐ²Ñ–Ñ‚Ð°: {assessment.education_level}")
        if assessment.work_preferences:
            context_parts.append(f"- Ð Ð¾Ð±Ð¾Ñ‡Ñ– Ð¿ÐµÑ€ÐµÐ²Ð°Ð³Ð¸: {assessment.work_preferences}")
        
        # Goals
        current_goals = assessment.answers.get('current_goals')
        if current_goals:
            context_parts.append(f"- ÐšÐ¾Ñ€Ð¾Ñ‚ÐºÐ¾ÑÑ‚Ñ€Ð¾ÐºÐ¾Ð²Ñ– Ñ†Ñ–Ð»Ñ–: {current_goals}")
        long_term = assessment.answers.get('long_term_goals')
        if long_term:
            context_parts.append(f"- Ð”Ð¾Ð²Ð³Ð¾ÑÑ‚Ñ€Ð¾ÐºÐ¾Ð²Ñ– Ñ†Ñ–Ð»Ñ–: {long_term}")
        
        # Add other important fields
        if assessment.leadership_experience:
            context_parts.append("- ÐœÐ°Ñ” Ð´Ð¾ÑÐ²Ñ–Ð´ Ð»Ñ–Ð´ÐµÑ€ÑÑ‚Ð²Ð°")
        if assessment.civilian_certifications:
            context_parts.append(f"- Ð¡ÐµÑ€Ñ‚Ð¸Ñ„Ñ–ÐºÐ°Ñ†Ñ–Ñ—: {assessment.civilian_certifications}")
        if assessment.locality:
            context_parts.append(f"- Ð ÐµÐ³Ñ–Ð¾Ð½: {assessment.locality}")
        
        return "\n".join(context_parts) + "\n"

    @staticmethod
    def _search_knowledge_base(query, max_results=3):
        """Search for relevant documents and articles in the knowledge base."""
        try:
            from api.models.knowledge import KnowledgeDocument
            from api.models.article import Article
            from django.db.models import Q
            
            # Simple keyword search
            search_terms = query.lower().split()[:3]
            
            results = []
            
            # Search in KnowledgeDocuments
            q_objects = Q()
            for term in search_terms:
                q_objects |= Q(title__icontains=term) | Q(raw_text_content__icontains=term)
            
            docs = KnowledgeDocument.objects.filter(q_objects).distinct()[:max_results]
            
            for doc in docs:
                content_snippet = doc.raw_text_content[:300] + "..." if len(doc.raw_text_content) > 300 else doc.raw_text_content
                results.append({
                    'title': doc.title,
                    'summary': content_snippet,
                    'url': doc.source_url or '',
                    'type': 'document'
                })
            
            # Also search in published Articles
            if len(results) < max_results:
                remaining = max_results - len(results)
                q_articles = Q(is_published=True)
                for term in search_terms:
                    q_articles &= Q(title__icontains=term) | Q(content__icontains=term)
                
                articles = Article.objects.filter(q_articles).distinct()[:remaining]
                
                for article in articles:
                    content_snippet = article.content[:300] + "..." if len(article.content) > 300 else article.content
                    results.append({
                        'title': article.title,
                        'summary': content_snippet,
                        'url': f'/articles/{article.slug}',
                        'type': 'article'
                    })
            
            return results
        except Exception as e:
            logger = logging.getLogger(__name__)
            logger.exception('Error searching knowledge base')
            return []

    @staticmethod
    def _process_response(assessment, raw_text):
        """
        Parses the raw LLM response for JSON updates, applies them to the assessment,
        and returns the clean text to show to the user.
        Works for ALL conversation types now, not just assessment mode.
        """
        # Parse for JSON updates
        json_match = re.search(r'```json\s*(\{.*?\})\s*```', raw_text, re.DOTALL)

        if not json_match:
            return raw_text

        # Always try to strip the JSON block first so the user doesn't see it
        clean_text = raw_text.replace(json_match.group(0), '').strip()

        try:
            data = json.loads(json_match.group(1))
            updates = data.get('updates', {})
            if updates:
                if not assessment.answers:
                    assessment.answers = {}
                assessment.answers.update(updates)
                assessment.save()

            return clean_text
        except Exception as e:
            logger = logging.getLogger(__name__)
            logger.exception('Failed to parse or save JSON updates from LLM response')
            # Even if saving fails, return the clean text without the JSON block
            return clean_text

    @staticmethod
    def generate_initial_message(user, conversation):
        """
        Generate an initial assistant message for a newly created conversation.
        """
        api_key = getattr(settings, 'GOOGLE_API_KEY', None) or os.environ.get('GOOGLE_API_KEY')
        if not api_key:
            return "Ð’Ñ–Ñ‚Ð°ÑŽ! Ð¯ Ð²Ð°Ñˆ ÐºÐ°Ñ€'Ñ”Ñ€Ð½Ð¸Ð¹ Ñ€Ð°Ð´Ð½Ð¸Ðº. Ð Ð°Ð´Ð¸Ð¹(Ð°), Ñ‰Ð¾ Ð²Ð¸ Ñ‚ÑƒÑ‚. Ð§Ð¸Ð¼ Ð¼Ð¾Ð¶Ñƒ Ð´Ð¾Ð¿Ð¾Ð¼Ð¾Ð³Ñ‚Ð¸?"

        try:
            genai.configure(api_key=api_key)
            model_name = getattr(settings, 'GOOGLE_LLM_MODEL', 'models/gemin-2.5-flash')

            # Get or create assessment
            try:
                assessment, _ = UserAssessment.objects.get_or_create(user=user)
            except MultipleObjectsReturned:
                assessments = UserAssessment.objects.filter(user=user).order_by('-updated_at')
                assessment = assessments.first()

            # Get the appropriate system prompt based on conversation type
            conv_type = conversation.conv_type
            if conv_type and conv_type in AdvisorService.SYSTEM_PROMPTS:
                system_prompt = AdvisorService.SYSTEM_PROMPTS[conv_type]
            else:
                system_prompt = AdvisorService.SYSTEM_PROMPTS['assessment']
            
            # Add user context
            user_context = AdvisorService._format_assessment_context(assessment)
            
            prompt = f"""{system_prompt}
{user_context}

ÐšÐ¾Ñ€Ð¾Ñ‚ÐºÐ¾ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ñ‚ÐµÑÑŒ Ñ– Ð¿Ð¾ÑÑ‚Ð°Ð²Ñ‚Ðµ Ð»Ð°ÐºÐ¾Ð½Ñ–Ñ‡Ð½Ðµ Ð²ÑÑ‚ÑƒÐ¿Ð½Ðµ Ð¿Ð¸Ñ‚Ð°Ð½Ð½Ñ Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´Ð½Ð¾ Ð´Ð¾ Ð²Ð°ÑˆÐ¾Ñ— Ñ€Ð¾Ð»Ñ– Ñ‚Ð° Ð¿Ñ€Ð¾Ñ„Ñ–Ð»ÑŽ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡Ð°.
"""

            model = genai.GenerativeModel(model_name)
            response = model.generate_content(prompt)

            if not response.parts:
                return "(ÐÐµÐ¼Ð°Ñ” Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´Ñ– Ð²Ñ–Ð´ LLM)"

            return response.text
        except Exception as e:
            logger = logging.getLogger(__name__)
            logger.exception('Error generating initial message from LLM; returning fallback greeting')
            return "Ð’Ñ–Ñ‚Ð°ÑŽ! Ð¯ Ð²Ð°Ñˆ ÐºÐ°Ñ€'Ñ”Ñ€Ð½Ð¸Ð¹ Ñ€Ð°Ð´Ð½Ð¸Ðº. Ð Ð°Ð´Ð¸Ð¹(Ð°), Ñ‰Ð¾ Ð²Ð¸ Ñ‚ÑƒÑ‚. Ð§Ð¸Ð¼ Ð¼Ð¾Ð¶Ñƒ Ð´Ð¾Ð¿Ð¾Ð¼Ð¾Ð³Ñ‚Ð¸?"

    @staticmethod
    def generate_conversation_title(conversation):
        """
        Generate a short, descriptive title for the conversation based on the first 3 exchanges.
        Called after the 3rd user message.
        """
        api_key = getattr(settings, 'GOOGLE_API_KEY', None) or os.environ.get('GOOGLE_API_KEY')
        if not api_key:
            return  # Skip if no LLM configured
        
        try:
            genai.configure(api_key=api_key)
            model_name = getattr(settings, 'GOOGLE_LLM_MODEL', 'models/gemin-2.5-flash')
            
            # Get the first 6 messages (3 user + 3 AI)
            from api.models.message import Message
            messages = conversation.messages.order_by('created_at')[:6]
            
            # Build conversation summary
            conversation_text = ""
            for msg in messages:
                role = "ÐšÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡" if msg.is_user else "Ð Ð°Ð´Ð½Ð¸Ðº"
                # Truncate long messages
                content = msg.content[:200] if len(msg.content) > 200 else msg.content
                conversation_text += f"{role}: {content}\n"
            
            # Get conversation type label
            conv_type_label = ""
            if conversation.conv_type:
                conv_type_label = dict(ConversationType.choices).get(conversation.conv_type, "")
            
            prompt = f"""ÐÐ° Ð¾ÑÐ½Ð¾Ð²Ñ– Ñ†Ñ–Ñ”Ñ— Ñ€Ð¾Ð·Ð¼Ð¾Ð²Ð¸ ÑÑ‚Ð²Ð¾Ñ€Ñ–Ñ‚ÑŒ Ð”Ð£Ð–Ð• ÐšÐžÐ ÐžÐ¢ÐšÐ£ Ð½Ð°Ð·Ð²Ñƒ (Ð¼Ð°ÐºÑÐ¸Ð¼ÑƒÐ¼ 2-3 ÑÐ»Ð¾Ð²Ð°).
ÐÐ°Ð·Ð²Ð° Ð¼Ð°Ñ” Ð²Ñ–Ð´Ð¾Ð±Ñ€Ð°Ð¶Ð°Ñ‚Ð¸ ÐžÐ¡ÐÐžÐ’ÐÐ£ Ð¢Ð•ÐœÐ£ Ñ€Ð¾Ð·Ð¼Ð¾Ð²Ð¸.

Ð¢Ð¸Ð¿ Ñ€Ð¾Ð·Ð¼Ð¾Ð²Ð¸: {conv_type_label or 'Ð—Ð°Ð³Ð°Ð»ÑŒÐ½Ð° ÐºÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ñ†Ñ–Ñ'}

Ð Ð¾Ð·Ð¼Ð¾Ð²Ð°:
{conversation_text}

Ð’Ð˜ÐœÐžÐ“Ð˜:
- ÐœÐ°ÐºÑÐ¸Ð¼ÑƒÐ¼ 4-5 ÑÐ»Ñ–Ð²
- Ð£ÐºÑ€Ð°Ñ—Ð½ÑÑŒÐºÐ¾ÑŽ Ð¼Ð¾Ð²Ð¾ÑŽ
- Ð‘Ð•Ð— Ð»Ð°Ð¿Ð¾Ðº, Ð‘Ð•Ð— Ð¿Ñ€ÐµÑ„Ñ–ÐºÑÑ–Ð² Ñ‚Ð¸Ð¿Ñƒ "ÐÐ°Ð·Ð²Ð°:", Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ñ‚ÐµÐºÑÑ‚
- ÐžÐ¿Ð¸ÑÑƒÑ” Ð¡Ð£Ð¢Ð¬ Ñ€Ð¾Ð·Ð¼Ð¾Ð²Ð¸ (Ð½Ð°Ð¿Ñ€Ð¸ÐºÐ»Ð°Ð´: "ÐŸÐ¾ÑˆÑƒÐº Ñ€Ð¾Ð±Ð¾Ñ‚Ð¸ Python developer", "Ð’Ð°Ð»Ñ–Ð´Ð°Ñ†Ñ–Ñ Ñ–Ð´ÐµÑ— Ñ€ÐµÑÑ‚Ð¾Ñ€Ð°Ð½Ñƒ", "ÐÐ°Ð²Ñ‡Ð°Ð½Ð½Ñ Ð²ÐµÐ±-Ñ€Ð¾Ð·Ñ€Ð¾Ð±Ñ†Ñ–")

ÐÐ°Ð·Ð²Ð°:"""

            model = genai.GenerativeModel(model_name)
            response = model.generate_content(prompt)
            
            if response.parts and response.text:
                # Clean up the response
                title = response.text.strip()
                # Remove quotes if present
                title = title.strip('"').strip("'").strip()
                # Limit length
                if len(title) > 60:
                    title = title[:57] + "..."
                
                # Update conversation
                conversation.title = title
                conversation.save(update_fields=['title'])
                
        except Exception as e:
            logger = logging.getLogger(__name__)
            logger.exception('Error generating conversation title')

    @staticmethod
    def generate_resume_content(user, resume, field, context=None):
        """
        Generates content for a specific resume field using AI.
        """
        api_key = getattr(settings, 'GOOGLE_API_KEY', None) or os.environ.get('GOOGLE_API_KEY')
        if not api_key:
            return "AI configuration missing."

        try:
            genai.configure(api_key=api_key)
            model_name = getattr(settings, 'GOOGLE_LLM_MODEL', 'models/gemin-2.5-flash')
            
            # Get user assessment
            try:
                assessment = UserAssessment.objects.get(user=user)
                assessment_text = f"""
                Skills: {assessment.primary_skills}
                Experience: {assessment.experience_years} years
                Preferences: {assessment.work_preferences}
                """
            except UserAssessment.DoesNotExist:
                assessment_text = "No assessment data available."

            # Build prompt based on field
            prompt = f"""
            You are an expert resume writer helping a Ukrainian veteran transition to a civilian career.
            
            User Profile:
            {assessment_text}
            
            Current Resume Title: {resume.title}
            
            Task: Write a professional and compelling content for the resume field: "{field}".
            """
            
            if context:
                prompt += f"\nContext/Details provided by user: {context}"
                
            if field == 'summary':
                prompt += "\nWrite a professional summary (3-5 sentences) highlighting the user's strengths and career goals."
            elif field == 'experience_description':
                prompt += "\nWrite a concise description of the work experience. Use MAX 3-4 bullet points. Focus on key achievements. Keep it brief."
            elif field == 'skills':
                prompt += "\nList relevant technical and soft skills based on the profile."
            
            prompt += "\nReturn ONLY the content for the field, no explanations or markdown formatting unless requested."

            model = genai.GenerativeModel(model_name)
            response = model.generate_content(prompt)
            
            return response.text.strip()

        except Exception as e:
            logging.getLogger(__name__).error(f"Error generating resume content: {e}")
            return "Failed to generate content."
